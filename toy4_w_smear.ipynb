{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "w = exp(-6.5*ob1) w/ smearing on all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.22/02\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import energyflow as ef\n",
    "import energyflow.archs\n",
    "import root_numpy\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import omnifold\n",
    "import modplot\n",
    "import ibu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_row = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ob1 = np.random.random_sample((n_row,))\n",
    "ob2 = np.arctan(ob1+1)\n",
    "obs1 = ob1*(1+np.random.normal(loc=0, scale=0.1, size=len(ob1)))\n",
    "obs2 = ob2*(1+np.random.normal(loc=0, scale=0.15, size=len(ob2)))\n",
    "w = np.exp(-6.5*ob1)\n",
    "relw = w/min(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.2449638492178e-05 0.9999779517807228\n",
      "0.7854343869044885 1.1071443081113448\n",
      "7.416121571027753e-05 1.3141270511370569\n",
      "0.22029107868137243 1.6869186517442716\n"
     ]
    }
   ],
   "source": [
    "print(min(ob1),max(ob1))\n",
    "print(min(ob2),max(ob2))\n",
    "print(min(obs1),max(obs1))\n",
    "print(min(obs2),max(obs2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "data.append(ob1)\n",
    "data.append(ob2)\n",
    "data.append(obs1)\n",
    "data.append(obs2)\n",
    "data.append(relw)\n",
    "\n",
    "df = pd.DataFrame(np.transpose(data), columns=['ob1','ob2','ob1s','ob2s','relw'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "odd = []\n",
    "even = []\n",
    "for i in range(0, df.shape[0]):\n",
    "    if (i%2 == 0):\n",
    "        even.append(i)\n",
    "    else:\n",
    "        odd.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.iloc[odd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.iloc[even]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = shuffle(df1.loc[df1.index.repeat(round(df1.relw))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = shuffle(df2.loc[df2.index.repeat(round(df2.relw))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smear(df, var):\n",
    "    if var in df.columns:\n",
    "        array = np.asarray(df[var])\n",
    "        array_new = (1+np.random.normal(loc=0, scale=0.001,size=len(array)))*array\n",
    "        df['new'+var]=array_new\n",
    "    arrays = np.asarray(df[var+'s'])\n",
    "    arrays_new = (1+np.random.normal(loc=0, scale=0.001,size=len(arrays)))*arrays\n",
    "    df['new'+var+'s']=arrays_new\n",
    "    print(\"Done smearing \" + var)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done smearing ob1\n",
      "Done smearing ob2\n",
      "Done smearing ob1\n",
      "Done smearing ob2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ob1</th>\n",
       "      <th>ob2</th>\n",
       "      <th>ob1s</th>\n",
       "      <th>ob2s</th>\n",
       "      <th>relw</th>\n",
       "      <th>newob1</th>\n",
       "      <th>newob1s</th>\n",
       "      <th>newob2</th>\n",
       "      <th>newob2s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2428</th>\n",
       "      <td>0.005510</td>\n",
       "      <td>0.788146</td>\n",
       "      <td>0.004452</td>\n",
       "      <td>0.818783</td>\n",
       "      <td>641.647332</td>\n",
       "      <td>0.005514</td>\n",
       "      <td>0.004454</td>\n",
       "      <td>0.786843</td>\n",
       "      <td>0.819027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16074</th>\n",
       "      <td>0.080281</td>\n",
       "      <td>0.823971</td>\n",
       "      <td>0.067778</td>\n",
       "      <td>0.771314</td>\n",
       "      <td>394.661110</td>\n",
       "      <td>0.080226</td>\n",
       "      <td>0.067746</td>\n",
       "      <td>0.823563</td>\n",
       "      <td>0.770212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15440</th>\n",
       "      <td>0.829660</td>\n",
       "      <td>1.070606</td>\n",
       "      <td>0.829479</td>\n",
       "      <td>0.737069</td>\n",
       "      <td>3.025475</td>\n",
       "      <td>0.827775</td>\n",
       "      <td>0.829277</td>\n",
       "      <td>1.069646</td>\n",
       "      <td>0.737054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6914</th>\n",
       "      <td>0.239123</td>\n",
       "      <td>0.891788</td>\n",
       "      <td>0.219654</td>\n",
       "      <td>0.981305</td>\n",
       "      <td>140.548981</td>\n",
       "      <td>0.239065</td>\n",
       "      <td>0.219748</td>\n",
       "      <td>0.893279</td>\n",
       "      <td>0.982989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18494</th>\n",
       "      <td>0.002827</td>\n",
       "      <td>0.786810</td>\n",
       "      <td>0.002905</td>\n",
       "      <td>0.728151</td>\n",
       "      <td>652.938012</td>\n",
       "      <td>0.002825</td>\n",
       "      <td>0.002903</td>\n",
       "      <td>0.787465</td>\n",
       "      <td>0.727901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6170</th>\n",
       "      <td>0.029243</td>\n",
       "      <td>0.799808</td>\n",
       "      <td>0.028330</td>\n",
       "      <td>0.865443</td>\n",
       "      <td>549.923499</td>\n",
       "      <td>0.029238</td>\n",
       "      <td>0.028339</td>\n",
       "      <td>0.798349</td>\n",
       "      <td>0.865828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10078</th>\n",
       "      <td>0.165697</td>\n",
       "      <td>0.861759</td>\n",
       "      <td>0.137822</td>\n",
       "      <td>0.765332</td>\n",
       "      <td>226.518427</td>\n",
       "      <td>0.166065</td>\n",
       "      <td>0.137751</td>\n",
       "      <td>0.860650</td>\n",
       "      <td>0.765186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11710</th>\n",
       "      <td>0.202049</td>\n",
       "      <td>0.876897</td>\n",
       "      <td>0.193526</td>\n",
       "      <td>0.856168</td>\n",
       "      <td>178.848072</td>\n",
       "      <td>0.202081</td>\n",
       "      <td>0.193320</td>\n",
       "      <td>0.874288</td>\n",
       "      <td>0.856484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7484</th>\n",
       "      <td>0.190514</td>\n",
       "      <td>0.872152</td>\n",
       "      <td>0.187294</td>\n",
       "      <td>1.161484</td>\n",
       "      <td>192.772839</td>\n",
       "      <td>0.190573</td>\n",
       "      <td>0.186970</td>\n",
       "      <td>0.872798</td>\n",
       "      <td>1.161874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17362</th>\n",
       "      <td>0.008201</td>\n",
       "      <td>0.789482</td>\n",
       "      <td>0.007802</td>\n",
       "      <td>1.077751</td>\n",
       "      <td>630.524368</td>\n",
       "      <td>0.008205</td>\n",
       "      <td>0.007791</td>\n",
       "      <td>0.788763</td>\n",
       "      <td>1.076800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1069427 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ob1       ob2      ob1s      ob2s        relw    newob1   newob1s  \\\n",
       "2428   0.005510  0.788146  0.004452  0.818783  641.647332  0.005514  0.004454   \n",
       "16074  0.080281  0.823971  0.067778  0.771314  394.661110  0.080226  0.067746   \n",
       "15440  0.829660  1.070606  0.829479  0.737069    3.025475  0.827775  0.829277   \n",
       "6914   0.239123  0.891788  0.219654  0.981305  140.548981  0.239065  0.219748   \n",
       "18494  0.002827  0.786810  0.002905  0.728151  652.938012  0.002825  0.002903   \n",
       "...         ...       ...       ...       ...         ...       ...       ...   \n",
       "6170   0.029243  0.799808  0.028330  0.865443  549.923499  0.029238  0.028339   \n",
       "10078  0.165697  0.861759  0.137822  0.765332  226.518427  0.166065  0.137751   \n",
       "11710  0.202049  0.876897  0.193526  0.856168  178.848072  0.202081  0.193320   \n",
       "7484   0.190514  0.872152  0.187294  1.161484  192.772839  0.190573  0.186970   \n",
       "17362  0.008201  0.789482  0.007802  1.077751  630.524368  0.008205  0.007791   \n",
       "\n",
       "         newob2   newob2s  \n",
       "2428   0.786843  0.819027  \n",
       "16074  0.823563  0.770212  \n",
       "15440  1.069646  0.737054  \n",
       "6914   0.893279  0.982989  \n",
       "18494  0.787465  0.727901  \n",
       "...         ...       ...  \n",
       "6170   0.798349  0.865828  \n",
       "10078  0.860650  0.765186  \n",
       "11710  0.874288  0.856484  \n",
       "7484   0.872798  1.161874  \n",
       "17362  0.788763  1.076800  \n",
       "\n",
       "[1069427 rows x 9 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smear(df3, 'ob1')\n",
    "smear(df3, 'ob2')\n",
    "smear(df4, 'ob1')\n",
    "smear(df4, 'ob2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ob1low = 0\n",
    "ob1high = 1.5\n",
    "ob2low = 0.4\n",
    "ob2high = 1.4\n",
    "nbin = 20\n",
    "binwidth1 = (ob1high-ob1low)/nbin\n",
    "binwidth2 = (ob2high-ob2low)/nbin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'detector ob2')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "\n",
    "ax1 = plt.subplot(221)\n",
    "y3 = np.histogram(df3['newob1'], range=(ob1low,ob1high), bins=nbin)[0]\n",
    "y4 = np.histogram(df4['newob1'], range=(ob1low,ob1high), bins=nbin)[0]\n",
    "xerr1 = (ob1high-ob1low)/(2*nbin)\n",
    "plt.errorbar(np.arange(ob1low+xerr1,ob1high+xerr1,binwidth1), y3/sum(y3), xerr=xerr1,ls='none', linewidth=1)\n",
    "plt.errorbar(np.arange(ob1low+xerr1,ob1high+xerr1,binwidth1), y4/sum(y4), xerr=xerr1,ls='none', linewidth=1)\n",
    "#plt.errorbar(np.arange(ptlow+xerr1,pthigh+xerr1), y3/sum(y3), xerr=xerr1, yerr=np.sqrt(y3)/sum(y3),ls='none', linewidth=1)\n",
    "#plt.errorbar(np.arange(ptlow+xerr1,pthigh+xerr1), y4/sum(y4), xerr=xerr1, yerr=np.sqrt(y4)/sum(y4),ls='none', linewidth=1)\n",
    "plt.xlabel('generator ob1')\n",
    "plt.yscale('log')\n",
    "\n",
    "ax2 = plt.subplot(222)\n",
    "y3 = np.histogram(df3['newob2'], range=(ob2low,ob2high), bins=nbin)[0]\n",
    "y4 = np.histogram(df4['newob2'], range=(ob2low,ob2high), bins=nbin)[0]\n",
    "xerr2 = (ob2high-ob2low)/(2*nbin)\n",
    "plt.errorbar(np.arange(ob2low+xerr2,ob2high+xerr2,binwidth2), y3/sum(y3), xerr=xerr2,ls='none', linewidth=1)\n",
    "plt.errorbar(np.arange(ob2low+xerr2,ob2high+xerr2,binwidth2), y4/sum(y4), xerr=xerr2,ls='none', linewidth=1)\n",
    "#plt.errorbar(np.arange(rglow+xerr1,rghigh+xerr1,2*xerr1), y3/sum(y3), xerr=xerr1,yerr=np.sqrt(y3)/sum(y3),ls='none',linewidth=1)\n",
    "#plt.errorbar(np.arange(rglow+xerr1,rghigh+xerr1,2*xerr1), y4/sum(y4), xerr=xerr1,yerr=np.sqrt(y4)/sum(y4),ls='none',linewidth=1)\n",
    "plt.xlabel('generator ob2')\n",
    "\n",
    "ax3 = plt.subplot(223)\n",
    "y3 = np.histogram(df3['newob1s'], range=(ob1low,ob1high), bins=nbin)[0]\n",
    "y4 = np.histogram(df4['newob1s'], range=(ob1low,ob1high), bins=nbin)[0]\n",
    "xerr1 = (ob1high-ob1low)/(2*nbin)\n",
    "plt.errorbar(np.arange(ob1low+xerr1,ob1high+xerr1,binwidth1), y3/sum(y3), xerr=xerr1,ls='none', linewidth=1)\n",
    "plt.errorbar(np.arange(ob1low+xerr1,ob1high+xerr1,binwidth1), y4/sum(y4), xerr=xerr1,ls='none', linewidth=1)\n",
    "#plt.errorbar(np.arange(mlow+xerr1,mhigh+xerr1), y3/sum(y3), xerr=xerr1, yerr=np.sqrt(y3)/sum(y3),ls='none', linewidth=1)\n",
    "#plt.errorbar(np.arange(mlow+xerr1,mhigh+xerr1), y4/sum(y4), xerr=xerr1, yerr=np.sqrt(y4)/sum(y4),ls='none', linewidth=1)\n",
    "plt.xlabel('detector ob1')\n",
    "plt.yscale('log')\n",
    "\n",
    "ax4 = plt.subplot(224)\n",
    "y3 = np.histogram(df3['newob2s'], range=(ob2low,ob2high), bins=nbin)[0]\n",
    "y4 = np.histogram(df4['newob2s'], range=(ob2low,ob2high), bins=nbin)[0]\n",
    "xerr2 = (ob2high-ob2low)/(2*nbin)\n",
    "plt.errorbar(np.arange(ob2low+xerr2,ob2high+xerr2,binwidth2), y3/sum(y3), xerr=xerr2,ls='none', linewidth=1)\n",
    "plt.errorbar(np.arange(ob2low+xerr2,ob2high+xerr2,binwidth2), y4/sum(y4), xerr=xerr2,ls='none', linewidth=1)\n",
    "#plt.errorbar(np.arange(ptlow+xerr1,pthigh+xerr1), y3/sum(y3), xerr=xerr1, yerr=np.sqrt(y3)/sum(y3),ls='none', linewidth=1)\n",
    "#plt.errorbar(np.arange(ptlow+xerr1,pthigh+xerr1), y4/sum(y4), xerr=xerr1, yerr=np.sqrt(y4)/sum(y4),ls='none', linewidth=1)\n",
    "plt.xlabel('detector ob2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1038153, 1069427)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.shape[0],df4.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = { 'pythia' : df3, 'data' : df4 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "664.7332045066404"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(relw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (4,4)\n",
    "plt.rcParams['figure.dpi'] = 120\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.size'] = '8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "itnum = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_multifold = ['newob1','newob2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = {}\n",
    "\n",
    "obs.setdefault('newob1', {}).update({\n",
    "    'func': lambda dset, s: np.asarray(datasets[dset]['newob1'+s]),\n",
    "    'nbins_det': 20, 'nbins_mc': 20,\n",
    "    'xlim': (ob1low, ob1high), 'ylim': (0.00001, 100),\n",
    "    'xlabel': r'ob1','symbol': 'ob1',\n",
    "    'ylabel': '' ,\n",
    "    'yscale': 'log'\n",
    "})\n",
    "\n",
    "obs.setdefault('newob2', {}).update({\n",
    "    'func': lambda dset, s: np.asarray(datasets[dset]['newob2'+s]),\n",
    "    'nbins_det': 20, 'nbins_mc': 20,\n",
    "    'xlim': (0.5, 1.5), 'ylim': (0.00001, 10),\n",
    "    'xlabel': r'ob2','symbol': 'ob2',\n",
    "    'ylabel': '' \n",
    "    #'yscale': 'log'\n",
    "})\n",
    "\n",
    "# additional histogram and plot style information\n",
    "hist_style = {'histtype': 'step', 'density': True, 'lw': 1, 'zorder': 2}\n",
    "gen_style = {'linestyle': '--', 'color': 'blue', 'lw': 1.15, 'label': 'PYTHIA truth'}\n",
    "truth_style = {'step': 'mid', 'edgecolor': 'green', 'facecolor': (0.75, 0.875, 0.75),\n",
    "               'lw': 1.25, 'zorder': 0, 'label': '``Truth\\\"'}\n",
    "ibu_style = {'ls': '-', 'marker': 'o', 'ms': 2.5, 'color': 'gray', 'zorder': 1}\n",
    "omnifold_style = {'ls': '-', 'marker': 's', 'ms': 2.5, 'color': 'tab:red', 'zorder': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00469714 0.02522937 0.09286944 ... 0.19688166 0.22496417 0.10706803]\n",
      "Done with newob1\n",
      "[0.78778189 0.79731154 0.8293757  ... 0.87478437 0.88627636 0.83582654]\n",
      "Done with newob2\n"
     ]
    }
   ],
   "source": [
    "# calculate quantities to be stored in obs\n",
    "for obkey,ob in obs.items():\n",
    "    \n",
    "    # calculate observable for GEN, SIM, DATA, and TRUE\n",
    "    ob['genobs'], ob['simobs'] = ob['func']('pythia', ''), ob['func']('pythia', 's')\n",
    "    ob['truthobs'], ob['dataobs'] = ob['func']('data', ''), ob['func']('data', 's')\n",
    "    \n",
    "    print(ob['genobs'])\n",
    "    # setup bins\n",
    "    ob['bins_det'] = np.linspace(ob['xlim'][0], ob['xlim'][1], ob['nbins_det']+1)\n",
    "    ob['bins_mc'] = np.linspace(ob['xlim'][0], ob['xlim'][1], ob['nbins_mc']+1)\n",
    "    ob['midbins_det'] = (ob['bins_det'][:-1] + ob['bins_det'][1:])/2\n",
    "    ob['midbins_mc'] = (ob['bins_mc'][:-1] + ob['bins_mc'][1:])/2\n",
    "    ob['binwidth_det'] = ob['bins_det'][1] - ob['bins_det'][0]\n",
    "    ob['binwidth_mc'] = ob['bins_mc'][1] - ob['bins_mc'][0]\n",
    "    \n",
    "    # get the histograms of GEN, DATA, and TRUTH level observables\n",
    "    ob['genobs_hist'] = np.histogram(ob['genobs'], bins=ob['bins_mc'], density=True)[0]\n",
    "    ob['data_hist'] = np.histogram(ob['dataobs'], bins=ob['bins_det'], density=True)[0]\n",
    "    ob['truth_hist'], ob['truth_hist_unc'] = modplot.calc_hist(ob['truthobs'], bins=ob['bins_mc'], \n",
    "                                                               density=True)[:2]\n",
    "\n",
    "    # compute (and normalize) the response matrix between GEN and SIM\n",
    "    ob['response'] = np.histogram2d(ob['simobs'], ob['genobs'], bins=(ob['bins_det'], ob['bins_mc']))[0]\n",
    "    ob['response'] /= (ob['response'].sum(axis=0) + 10**-50)\n",
    "    \n",
    "    # perform iterative Bayesian unfolding\n",
    "    ob['ibu_phis'] = ibu.ibu(ob['data_hist'], ob['response'], ob['genobs_hist'], \n",
    "                         ob['binwidth_det'], ob['binwidth_mc'], it=itnum)\n",
    "    ob['ibu_phi_unc'] = ibu.ibu_unc(ob, it=itnum, nresamples=25)\n",
    "    \n",
    "    print('Done with', obkey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_layer_sizes = [100, 100]\n",
    "model_layer_sizes = [100, 100, 100] # use this for the full network size\n",
    "\n",
    "# set up the array of data/simulation detector-level observables\n",
    "X_det = np.asarray([np.concatenate((obs[obkey]['dataobs'], obs[obkey]['simobs'])) for obkey in obs_multifold]).T\n",
    "Y_det = ef.utils.to_categorical(np.concatenate((np.ones(len(obs['newob1']['dataobs'])), \n",
    "                                                np.zeros(len(obs['newob1']['simobs'])))))\n",
    "\n",
    "# set up the array of generation particle-level observables\n",
    "X_gen = np.asarray([np.concatenate((obs[obkey]['genobs'], obs[obkey]['genobs'])) for obkey in obs_multifold]).T\n",
    "Y_gen = ef.utils.to_categorical(np.concatenate((np.ones(len(obs['newob1']['genobs'])), \n",
    "                                                np.zeros(len(obs['newob1']['genobs'])))))\n",
    "\n",
    "# standardize the inputs\n",
    "X_det = (X_det - np.mean(X_det, axis=0))/np.std(X_det, axis=0)\n",
    "X_gen = (X_gen - np.mean(X_gen, axis=0))/np.std(X_gen, axis=0)\n",
    "\n",
    "# Specify the training parameters\n",
    "# model parameters for the Step 1 network\n",
    "det_args = {'input_dim': len(obs_multifold), 'dense_sizes': model_layer_sizes,\n",
    "            'patience': 10, 'filepath': 'Step1_{}', 'save_weights_only': False, \n",
    "            'modelcheck_opts': {'save_best_only': True, 'verbose': 1}}\n",
    "\n",
    "# model parameters for the Step 2 network\n",
    "mc_args = {'input_dim': len(obs_multifold), 'dense_sizes': model_layer_sizes, \n",
    "           'patience': 10, 'filepath': 'Step2_{}', 'save_weights_only': False, \n",
    "           'modelcheck_opts': {'save_best_only': True, 'verbose': 1}}\n",
    "\n",
    "# general training parameters\n",
    "#fitargs = {'batch_size': 500, 'epochs': 2, 'verbose': 1}\n",
    "fitargs = {'batch_size': 500, 'epochs': 100, 'verbose': 1} # use this for a full training\n",
    "\n",
    "# reweight the sim and data to have the same total weight to begin with\n",
    "ndata, nsim = np.count_nonzero(\n",
    "    Y_det[:,1]), np.count_nonzero(Y_det[:,0])\n",
    "wdata = np.ones(ndata)\n",
    "winit = ndata/nsim*np.ones(nsim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 2)]               0         \n",
      "_________________________________________________________________\n",
      "dense_0 (Dense)              (None, 100)               300       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 2)                 202       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 20,702\n",
      "Trainable params: 20,702\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"functional_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 2)]               0         \n",
      "_________________________________________________________________\n",
      "dense_0 (Dense)              (None, 100)               300       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 2)                 202       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 20,702\n",
      "Trainable params: 20,702\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "3369/3373 [============================>.] - ETA: 0s - loss: 0.7015 - acc: 0.5266\n",
      "Epoch 00001: val_loss improved from inf to 0.69878, saving model to Step1_0_Epoch-1\n",
      "WARNING:tensorflow:From /gpfs/loomis/project/caines/ys668/conda_envs/my_env/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /gpfs/loomis/project/caines/ys668/conda_envs/my_env/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: Step1_0_Epoch-1/assets\n",
      "3373/3373 [==============================] - 50s 15ms/step - loss: 0.7014 - acc: 0.5266 - val_loss: 0.6988 - val_acc: 0.5273\n",
      "Epoch 2/100\n",
      "3359/3373 [============================>.] - ETA: 0s - loss: 0.6954 - acc: 0.5433\n",
      "Epoch 00002: val_loss improved from 0.69878 to 0.69136, saving model to Step1_0_Epoch-2\n",
      "INFO:tensorflow:Assets written to: Step1_0_Epoch-2/assets\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.6954 - acc: 0.5432 - val_loss: 0.6914 - val_acc: 0.5512\n",
      "Epoch 3/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.6860 - acc: 0.5642\n",
      "Epoch 00003: val_loss improved from 0.69136 to 0.68003, saving model to Step1_0_Epoch-3\n",
      "INFO:tensorflow:Assets written to: Step1_0_Epoch-3/assets\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.6859 - acc: 0.5642 - val_loss: 0.6800 - val_acc: 0.5730\n",
      "Epoch 4/100\n",
      "3360/3373 [============================>.] - ETA: 0s - loss: 0.6746 - acc: 0.5816\n",
      "Epoch 00004: val_loss improved from 0.68003 to 0.67047, saving model to Step1_0_Epoch-4\n",
      "INFO:tensorflow:Assets written to: Step1_0_Epoch-4/assets\n",
      "3373/3373 [==============================] - 13s 4ms/step - loss: 0.6745 - acc: 0.5817 - val_loss: 0.6705 - val_acc: 0.5833\n",
      "Epoch 5/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.6648 - acc: 0.5923\n",
      "Epoch 00005: val_loss improved from 0.67047 to 0.65913, saving model to Step1_0_Epoch-5\n",
      "INFO:tensorflow:Assets written to: Step1_0_Epoch-5/assets\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.6648 - acc: 0.5924 - val_loss: 0.6591 - val_acc: 0.5996\n",
      "Epoch 6/100\n",
      "3360/3373 [============================>.] - ETA: 0s - loss: 0.6563 - acc: 0.6021\n",
      "Epoch 00006: val_loss improved from 0.65913 to 0.65342, saving model to Step1_0_Epoch-6\n",
      "INFO:tensorflow:Assets written to: Step1_0_Epoch-6/assets\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.6563 - acc: 0.6021 - val_loss: 0.6534 - val_acc: 0.6009\n",
      "Epoch 7/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.6495 - acc: 0.6095\n",
      "Epoch 00007: val_loss improved from 0.65342 to 0.64846, saving model to Step1_0_Epoch-7\n",
      "INFO:tensorflow:Assets written to: Step1_0_Epoch-7/assets\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.6495 - acc: 0.6096 - val_loss: 0.6485 - val_acc: 0.6078\n",
      "Epoch 8/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.6434 - acc: 0.6162\n",
      "Epoch 00008: val_loss improved from 0.64846 to 0.64174, saving model to Step1_0_Epoch-8\n",
      "INFO:tensorflow:Assets written to: Step1_0_Epoch-8/assets\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.6434 - acc: 0.6162 - val_loss: 0.6417 - val_acc: 0.6111\n",
      "Epoch 9/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.6378 - acc: 0.6213\n",
      "Epoch 00009: val_loss improved from 0.64174 to 0.63652, saving model to Step1_0_Epoch-9\n",
      "INFO:tensorflow:Assets written to: Step1_0_Epoch-9/assets\n",
      "3373/3373 [==============================] - 13s 4ms/step - loss: 0.6378 - acc: 0.6213 - val_loss: 0.6365 - val_acc: 0.6207\n",
      "Epoch 10/100\n",
      "3360/3373 [============================>.] - ETA: 0s - loss: 0.6327 - acc: 0.6266\n",
      "Epoch 00010: val_loss improved from 0.63652 to 0.63081, saving model to Step1_0_Epoch-10\n",
      "INFO:tensorflow:Assets written to: Step1_0_Epoch-10/assets\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.6327 - acc: 0.6266 - val_loss: 0.6308 - val_acc: 0.6315\n",
      "Epoch 11/100\n",
      "3370/3373 [============================>.] - ETA: 0s - loss: 0.6278 - acc: 0.6308\n",
      "Epoch 00011: val_loss improved from 0.63081 to 0.62573, saving model to Step1_0_Epoch-11\n",
      "INFO:tensorflow:Assets written to: Step1_0_Epoch-11/assets\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.6278 - acc: 0.6308 - val_loss: 0.6257 - val_acc: 0.6382\n",
      "Epoch 12/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.6233 - acc: 0.6357\n",
      "Epoch 00012: val_loss improved from 0.62573 to 0.62401, saving model to Step1_0_Epoch-12\n",
      "INFO:tensorflow:Assets written to: Step1_0_Epoch-12/assets\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.6233 - acc: 0.6357 - val_loss: 0.6240 - val_acc: 0.6434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.6194 - acc: 0.6390\n",
      "Epoch 00013: val_loss did not improve from 0.62401\n",
      "3373/3373 [==============================] - 12s 3ms/step - loss: 0.6194 - acc: 0.6390 - val_loss: 0.6273 - val_acc: 0.6346\n",
      "Epoch 14/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.6155 - acc: 0.6431\n",
      "Epoch 00014: val_loss improved from 0.62401 to 0.61666, saving model to Step1_0_Epoch-14\n",
      "INFO:tensorflow:Assets written to: Step1_0_Epoch-14/assets\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.6155 - acc: 0.6431 - val_loss: 0.6167 - val_acc: 0.6405\n",
      "Epoch 15/100\n",
      "3360/3373 [============================>.] - ETA: 0s - loss: 0.6115 - acc: 0.6467\n",
      "Epoch 00015: val_loss improved from 0.61666 to 0.61208, saving model to Step1_0_Epoch-15\n",
      "INFO:tensorflow:Assets written to: Step1_0_Epoch-15/assets\n",
      "3373/3373 [==============================] - 13s 4ms/step - loss: 0.6115 - acc: 0.6467 - val_loss: 0.6121 - val_acc: 0.6400\n",
      "Epoch 16/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.6083 - acc: 0.6499\n",
      "Epoch 00016: val_loss improved from 0.61208 to 0.60308, saving model to Step1_0_Epoch-16\n",
      "INFO:tensorflow:Assets written to: Step1_0_Epoch-16/assets\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.6083 - acc: 0.6499 - val_loss: 0.6031 - val_acc: 0.6573\n",
      "Epoch 17/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.6046 - acc: 0.6530\n",
      "Epoch 00017: val_loss did not improve from 0.60308\n",
      "3373/3373 [==============================] - 12s 3ms/step - loss: 0.6046 - acc: 0.6531 - val_loss: 0.6032 - val_acc: 0.6562\n",
      "Epoch 18/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.6012 - acc: 0.6562\n",
      "Epoch 00018: val_loss improved from 0.60308 to 0.59972, saving model to Step1_0_Epoch-18\n",
      "INFO:tensorflow:Assets written to: Step1_0_Epoch-18/assets\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.6012 - acc: 0.6562 - val_loss: 0.5997 - val_acc: 0.6554\n",
      "Epoch 19/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5984 - acc: 0.6582\n",
      "Epoch 00019: val_loss improved from 0.59972 to 0.59908, saving model to Step1_0_Epoch-19\n",
      "INFO:tensorflow:Assets written to: Step1_0_Epoch-19/assets\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.5984 - acc: 0.6582 - val_loss: 0.5991 - val_acc: 0.6583\n",
      "Epoch 20/100\n",
      "3366/3373 [============================>.] - ETA: 0s - loss: 0.5948 - acc: 0.6616\n",
      "Epoch 00020: val_loss improved from 0.59908 to 0.59193, saving model to Step1_0_Epoch-20\n",
      "INFO:tensorflow:Assets written to: Step1_0_Epoch-20/assets\n",
      "3373/3373 [==============================] - 13s 4ms/step - loss: 0.5948 - acc: 0.6616 - val_loss: 0.5919 - val_acc: 0.6650\n",
      "Epoch 21/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5922 - acc: 0.6635\n",
      "Epoch 00021: val_loss improved from 0.59193 to 0.58810, saving model to Step1_0_Epoch-21\n",
      "INFO:tensorflow:Assets written to: Step1_0_Epoch-21/assets\n",
      "3373/3373 [==============================] - 13s 4ms/step - loss: 0.5922 - acc: 0.6635 - val_loss: 0.5881 - val_acc: 0.6652\n",
      "Epoch 22/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5891 - acc: 0.6662\n",
      "Epoch 00022: val_loss did not improve from 0.58810\n",
      "3373/3373 [==============================] - 12s 3ms/step - loss: 0.5891 - acc: 0.6662 - val_loss: 0.5930 - val_acc: 0.6652\n",
      "Epoch 23/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5866 - acc: 0.6687\n",
      "Epoch 00023: val_loss did not improve from 0.58810\n",
      "3373/3373 [==============================] - 12s 3ms/step - loss: 0.5866 - acc: 0.6687 - val_loss: 0.5893 - val_acc: 0.6662\n",
      "Epoch 24/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5836 - acc: 0.6710\n",
      "Epoch 00024: val_loss improved from 0.58810 to 0.58212, saving model to Step1_0_Epoch-24\n",
      "INFO:tensorflow:Assets written to: Step1_0_Epoch-24/assets\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.5836 - acc: 0.6710 - val_loss: 0.5821 - val_acc: 0.6732\n",
      "Epoch 25/100\n",
      "3357/3373 [============================>.] - ETA: 0s - loss: 0.5811 - acc: 0.6728\n",
      "Epoch 00025: val_loss improved from 0.58212 to 0.58122, saving model to Step1_0_Epoch-25\n",
      "INFO:tensorflow:Assets written to: Step1_0_Epoch-25/assets\n",
      "3373/3373 [==============================] - 13s 4ms/step - loss: 0.5811 - acc: 0.6728 - val_loss: 0.5812 - val_acc: 0.6713\n",
      "Epoch 26/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5790 - acc: 0.6745\n",
      "Epoch 00026: val_loss improved from 0.58122 to 0.57689, saving model to Step1_0_Epoch-26\n",
      "INFO:tensorflow:Assets written to: Step1_0_Epoch-26/assets\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.5790 - acc: 0.6745 - val_loss: 0.5769 - val_acc: 0.6757\n",
      "Epoch 27/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5765 - acc: 0.6760\n",
      "Epoch 00027: val_loss improved from 0.57689 to 0.57560, saving model to Step1_0_Epoch-27\n",
      "INFO:tensorflow:Assets written to: Step1_0_Epoch-27/assets\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.5766 - acc: 0.6760 - val_loss: 0.5756 - val_acc: 0.6750\n",
      "Epoch 28/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5744 - acc: 0.6777\n",
      "Epoch 00028: val_loss improved from 0.57560 to 0.57322, saving model to Step1_0_Epoch-28\n",
      "INFO:tensorflow:Assets written to: Step1_0_Epoch-28/assets\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.5744 - acc: 0.6777 - val_loss: 0.5732 - val_acc: 0.6816\n",
      "Epoch 29/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5720 - acc: 0.6797\n",
      "Epoch 00029: val_loss improved from 0.57322 to 0.57103, saving model to Step1_0_Epoch-29\n",
      "INFO:tensorflow:Assets written to: Step1_0_Epoch-29/assets\n",
      "3373/3373 [==============================] - 13s 4ms/step - loss: 0.5720 - acc: 0.6797 - val_loss: 0.5710 - val_acc: 0.6838\n",
      "Epoch 30/100\n",
      "3366/3373 [============================>.] - ETA: 0s - loss: 0.5696 - acc: 0.6819\n",
      "Epoch 00030: val_loss improved from 0.57103 to 0.56676, saving model to Step1_0_Epoch-30\n",
      "INFO:tensorflow:Assets written to: Step1_0_Epoch-30/assets\n",
      "3373/3373 [==============================] - 13s 4ms/step - loss: 0.5696 - acc: 0.6818 - val_loss: 0.5668 - val_acc: 0.6848\n",
      "Epoch 31/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5678 - acc: 0.6831\n",
      "Epoch 00031: val_loss improved from 0.56676 to 0.56293, saving model to Step1_0_Epoch-31\n",
      "INFO:tensorflow:Assets written to: Step1_0_Epoch-31/assets\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.5678 - acc: 0.6831 - val_loss: 0.5629 - val_acc: 0.6883\n",
      "Epoch 32/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5651 - acc: 0.6853\n",
      "Epoch 00032: val_loss did not improve from 0.56293\n",
      "3373/3373 [==============================] - 12s 3ms/step - loss: 0.5651 - acc: 0.6853 - val_loss: 0.5710 - val_acc: 0.6811\n",
      "Epoch 33/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5636 - acc: 0.6864\n",
      "Epoch 00033: val_loss did not improve from 0.56293\n",
      "3373/3373 [==============================] - 12s 3ms/step - loss: 0.5636 - acc: 0.6864 - val_loss: 0.5636 - val_acc: 0.6843\n",
      "Epoch 34/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5611 - acc: 0.6886\n",
      "Epoch 00034: val_loss improved from 0.56293 to 0.55732, saving model to Step1_0_Epoch-34\n",
      "INFO:tensorflow:Assets written to: Step1_0_Epoch-34/assets\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.5611 - acc: 0.6886 - val_loss: 0.5573 - val_acc: 0.6905\n",
      "Epoch 35/100\n",
      "3360/3373 [============================>.] - ETA: 0s - loss: 0.5598 - acc: 0.6894\n",
      "Epoch 00035: val_loss did not improve from 0.55732\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.5598 - acc: 0.6895 - val_loss: 0.5655 - val_acc: 0.6826\n",
      "Epoch 36/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5573 - acc: 0.6915\n",
      "Epoch 00036: val_loss did not improve from 0.55732\n",
      "3373/3373 [==============================] - 12s 3ms/step - loss: 0.5573 - acc: 0.6915 - val_loss: 0.5577 - val_acc: 0.6904\n",
      "Epoch 37/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5561 - acc: 0.6919\n",
      "Epoch 00037: val_loss improved from 0.55732 to 0.55169, saving model to Step1_0_Epoch-37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Step1_0_Epoch-37/assets\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.5561 - acc: 0.6920 - val_loss: 0.5517 - val_acc: 0.6965\n",
      "Epoch 38/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5543 - acc: 0.6937\n",
      "Epoch 00038: val_loss did not improve from 0.55169\n",
      "3373/3373 [==============================] - 12s 3ms/step - loss: 0.5543 - acc: 0.6937 - val_loss: 0.5601 - val_acc: 0.6866\n",
      "Epoch 39/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5527 - acc: 0.6950\n",
      "Epoch 00039: val_loss improved from 0.55169 to 0.54288, saving model to Step1_0_Epoch-39\n",
      "INFO:tensorflow:Assets written to: Step1_0_Epoch-39/assets\n",
      "3373/3373 [==============================] - 13s 4ms/step - loss: 0.5526 - acc: 0.6950 - val_loss: 0.5429 - val_acc: 0.7027\n",
      "Epoch 40/100\n",
      "3365/3373 [============================>.] - ETA: 0s - loss: 0.5508 - acc: 0.6963\n",
      "Epoch 00040: val_loss did not improve from 0.54288\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.5508 - acc: 0.6963 - val_loss: 0.5472 - val_acc: 0.6997\n",
      "Epoch 41/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5494 - acc: 0.6973\n",
      "Epoch 00041: val_loss did not improve from 0.54288\n",
      "3373/3373 [==============================] - 12s 3ms/step - loss: 0.5494 - acc: 0.6973 - val_loss: 0.5461 - val_acc: 0.6978\n",
      "Epoch 42/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5479 - acc: 0.6986\n",
      "Epoch 00042: val_loss did not improve from 0.54288\n",
      "3373/3373 [==============================] - 12s 3ms/step - loss: 0.5479 - acc: 0.6986 - val_loss: 0.5494 - val_acc: 0.7000\n",
      "Epoch 43/100\n",
      "3360/3373 [============================>.] - ETA: 0s - loss: 0.5461 - acc: 0.6995\n",
      "Epoch 00043: val_loss did not improve from 0.54288\n",
      "3373/3373 [==============================] - 12s 3ms/step - loss: 0.5461 - acc: 0.6995 - val_loss: 0.5515 - val_acc: 0.6955\n",
      "Epoch 44/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5451 - acc: 0.7003\n",
      "Epoch 00044: val_loss did not improve from 0.54288\n",
      "3373/3373 [==============================] - 12s 3ms/step - loss: 0.5451 - acc: 0.7003 - val_loss: 0.5521 - val_acc: 0.6957\n",
      "Epoch 45/100\n",
      "3360/3373 [============================>.] - ETA: 0s - loss: 0.5437 - acc: 0.7012\n",
      "Epoch 00045: val_loss improved from 0.54288 to 0.54038, saving model to Step1_0_Epoch-45\n",
      "INFO:tensorflow:Assets written to: Step1_0_Epoch-45/assets\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.5437 - acc: 0.7012 - val_loss: 0.5404 - val_acc: 0.7002\n",
      "Epoch 46/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5424 - acc: 0.7023\n",
      "Epoch 00046: val_loss did not improve from 0.54038\n",
      "3373/3373 [==============================] - 12s 3ms/step - loss: 0.5424 - acc: 0.7023 - val_loss: 0.5473 - val_acc: 0.6938\n",
      "Epoch 47/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5415 - acc: 0.7032\n",
      "Epoch 00047: val_loss did not improve from 0.54038\n",
      "3373/3373 [==============================] - 12s 3ms/step - loss: 0.5414 - acc: 0.7032 - val_loss: 0.5428 - val_acc: 0.6994\n",
      "Epoch 48/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5396 - acc: 0.7041\n",
      "Epoch 00048: val_loss did not improve from 0.54038\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.5396 - acc: 0.7040 - val_loss: 0.5626 - val_acc: 0.6863\n",
      "Epoch 49/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5382 - acc: 0.7055\n",
      "Epoch 00049: val_loss improved from 0.54038 to 0.53514, saving model to Step1_0_Epoch-49\n",
      "INFO:tensorflow:Assets written to: Step1_0_Epoch-49/assets\n",
      "3373/3373 [==============================] - 13s 4ms/step - loss: 0.5382 - acc: 0.7055 - val_loss: 0.5351 - val_acc: 0.7075\n",
      "Epoch 50/100\n",
      "3366/3373 [============================>.] - ETA: 0s - loss: 0.5371 - acc: 0.7056\n",
      "Epoch 00050: val_loss did not improve from 0.53514\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.5371 - acc: 0.7056 - val_loss: 0.5393 - val_acc: 0.7019\n",
      "Epoch 51/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5351 - acc: 0.7071\n",
      "Epoch 00051: val_loss did not improve from 0.53514\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.5351 - acc: 0.7071 - val_loss: 0.5391 - val_acc: 0.7052\n",
      "Epoch 52/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5346 - acc: 0.7073\n",
      "Epoch 00052: val_loss did not improve from 0.53514\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.5346 - acc: 0.7073 - val_loss: 0.5494 - val_acc: 0.6953\n",
      "Epoch 53/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5327 - acc: 0.7090\n",
      "Epoch 00053: val_loss improved from 0.53514 to 0.52846, saving model to Step1_0_Epoch-53\n",
      "INFO:tensorflow:Assets written to: Step1_0_Epoch-53/assets\n",
      "3373/3373 [==============================] - 13s 4ms/step - loss: 0.5327 - acc: 0.7090 - val_loss: 0.5285 - val_acc: 0.7129\n",
      "Epoch 54/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5327 - acc: 0.7086\n",
      "Epoch 00054: val_loss did not improve from 0.52846\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.5327 - acc: 0.7086 - val_loss: 0.5373 - val_acc: 0.7072\n",
      "Epoch 55/100\n",
      "3360/3373 [============================>.] - ETA: 0s - loss: 0.5308 - acc: 0.7106\n",
      "Epoch 00055: val_loss improved from 0.52846 to 0.52767, saving model to Step1_0_Epoch-55\n",
      "INFO:tensorflow:Assets written to: Step1_0_Epoch-55/assets\n",
      "3373/3373 [==============================] - 13s 4ms/step - loss: 0.5307 - acc: 0.7107 - val_loss: 0.5277 - val_acc: 0.7149\n",
      "Epoch 56/100\n",
      "3358/3373 [============================>.] - ETA: 0s - loss: 0.5297 - acc: 0.7105\n",
      "Epoch 00056: val_loss did not improve from 0.52767\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.5297 - acc: 0.7105 - val_loss: 0.5280 - val_acc: 0.7107\n",
      "Epoch 57/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5288 - acc: 0.7119\n",
      "Epoch 00057: val_loss did not improve from 0.52767\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.5288 - acc: 0.7119 - val_loss: 0.5369 - val_acc: 0.7092\n",
      "Epoch 58/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5276 - acc: 0.7123\n",
      "Epoch 00058: val_loss did not improve from 0.52767\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.5276 - acc: 0.7124 - val_loss: 0.5369 - val_acc: 0.7001\n",
      "Epoch 59/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5265 - acc: 0.7132\n",
      "Epoch 00059: val_loss did not improve from 0.52767\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.5265 - acc: 0.7131 - val_loss: 0.5371 - val_acc: 0.7065\n",
      "Epoch 60/100\n",
      "3366/3373 [============================>.] - ETA: 0s - loss: 0.5256 - acc: 0.7136\n",
      "Epoch 00060: val_loss improved from 0.52767 to 0.52512, saving model to Step1_0_Epoch-60\n",
      "INFO:tensorflow:Assets written to: Step1_0_Epoch-60/assets\n",
      "3373/3373 [==============================] - 13s 4ms/step - loss: 0.5256 - acc: 0.7136 - val_loss: 0.5251 - val_acc: 0.7127\n",
      "Epoch 61/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5245 - acc: 0.7143\n",
      "Epoch 00061: val_loss did not improve from 0.52512\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.5245 - acc: 0.7143 - val_loss: 0.5276 - val_acc: 0.7127\n",
      "Epoch 62/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5239 - acc: 0.7152\n",
      "Epoch 00062: val_loss improved from 0.52512 to 0.51758, saving model to Step1_0_Epoch-62\n",
      "INFO:tensorflow:Assets written to: Step1_0_Epoch-62/assets\n",
      "3373/3373 [==============================] - 13s 4ms/step - loss: 0.5239 - acc: 0.7152 - val_loss: 0.5176 - val_acc: 0.7192\n",
      "Epoch 63/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5229 - acc: 0.7156\n",
      "Epoch 00063: val_loss did not improve from 0.51758\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.5229 - acc: 0.7156 - val_loss: 0.5249 - val_acc: 0.7156\n",
      "Epoch 64/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5226 - acc: 0.7155\n",
      "Epoch 00064: val_loss did not improve from 0.51758\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.5226 - acc: 0.7155 - val_loss: 0.5276 - val_acc: 0.7129\n",
      "Epoch 65/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5211 - acc: 0.7167\n",
      "Epoch 00065: val_loss did not improve from 0.51758\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.5211 - acc: 0.7167 - val_loss: 0.5214 - val_acc: 0.7184\n",
      "Epoch 66/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5201 - acc: 0.7177\n",
      "Epoch 00066: val_loss did not improve from 0.51758\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.5201 - acc: 0.7177 - val_loss: 0.5248 - val_acc: 0.7161\n",
      "Epoch 67/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5189 - acc: 0.7180\n",
      "Epoch 00067: val_loss did not improve from 0.51758\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.5189 - acc: 0.7180 - val_loss: 0.5193 - val_acc: 0.7182\n",
      "Epoch 68/100\n",
      "3360/3373 [============================>.] - ETA: 0s - loss: 0.5181 - acc: 0.7187\n",
      "Epoch 00068: val_loss did not improve from 0.51758\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.5181 - acc: 0.7188 - val_loss: 0.5185 - val_acc: 0.7145\n",
      "Epoch 69/100\n",
      "3360/3373 [============================>.] - ETA: 0s - loss: 0.5181 - acc: 0.7187\n",
      "Epoch 00069: val_loss improved from 0.51758 to 0.51069, saving model to Step1_0_Epoch-69\n",
      "INFO:tensorflow:Assets written to: Step1_0_Epoch-69/assets\n",
      "3373/3373 [==============================] - 13s 4ms/step - loss: 0.5181 - acc: 0.7187 - val_loss: 0.5107 - val_acc: 0.7226\n",
      "Epoch 70/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5161 - acc: 0.7200\n",
      "Epoch 00070: val_loss did not improve from 0.51069\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.5161 - acc: 0.7200 - val_loss: 0.5211 - val_acc: 0.7160\n",
      "Epoch 71/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5153 - acc: 0.7207\n",
      "Epoch 00071: val_loss did not improve from 0.51069\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.5153 - acc: 0.7207 - val_loss: 0.5148 - val_acc: 0.7277\n",
      "Epoch 72/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5148 - acc: 0.7209\n",
      "Epoch 00072: val_loss did not improve from 0.51069\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.5148 - acc: 0.7209 - val_loss: 0.5239 - val_acc: 0.7161\n",
      "Epoch 73/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5139 - acc: 0.7211\n",
      "Epoch 00073: val_loss did not improve from 0.51069\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.5139 - acc: 0.7211 - val_loss: 0.5163 - val_acc: 0.7172\n",
      "Epoch 74/100\n",
      "3360/3373 [============================>.] - ETA: 0s - loss: 0.5124 - acc: 0.7222\n",
      "Epoch 00074: val_loss did not improve from 0.51069\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.5124 - acc: 0.7223 - val_loss: 0.5172 - val_acc: 0.7182\n",
      "Epoch 75/100\n",
      "3365/3373 [============================>.] - ETA: 0s - loss: 0.5115 - acc: 0.7231\n",
      "Epoch 00075: val_loss did not improve from 0.51069\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.5115 - acc: 0.7231 - val_loss: 0.5157 - val_acc: 0.7209\n",
      "Epoch 76/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5104 - acc: 0.7237\n",
      "Epoch 00076: val_loss improved from 0.51069 to 0.50716, saving model to Step1_0_Epoch-76\n",
      "INFO:tensorflow:Assets written to: Step1_0_Epoch-76/assets\n",
      "3373/3373 [==============================] - 13s 4ms/step - loss: 0.5105 - acc: 0.7237 - val_loss: 0.5072 - val_acc: 0.7236\n",
      "Epoch 77/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5101 - acc: 0.7237\n",
      "Epoch 00077: val_loss did not improve from 0.50716\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.5101 - acc: 0.7237 - val_loss: 0.5099 - val_acc: 0.7223\n",
      "Epoch 78/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5088 - acc: 0.7247\n",
      "Epoch 00078: val_loss did not improve from 0.50716\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.5089 - acc: 0.7246 - val_loss: 0.5110 - val_acc: 0.7220\n",
      "Epoch 79/100\n",
      "3360/3373 [============================>.] - ETA: 0s - loss: 0.5080 - acc: 0.7254\n",
      "Epoch 00079: val_loss did not improve from 0.50716\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.5080 - acc: 0.7254 - val_loss: 0.5190 - val_acc: 0.7180\n",
      "Epoch 80/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5082 - acc: 0.7251\n",
      "Epoch 00080: val_loss did not improve from 0.50716\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.5082 - acc: 0.7252 - val_loss: 0.5192 - val_acc: 0.7152\n",
      "Epoch 81/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5063 - acc: 0.7267\n",
      "Epoch 00081: val_loss did not improve from 0.50716\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.5063 - acc: 0.7267 - val_loss: 0.5093 - val_acc: 0.7231\n",
      "Epoch 82/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5051 - acc: 0.7272\n",
      "Epoch 00082: val_loss did not improve from 0.50716\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.5051 - acc: 0.7272 - val_loss: 0.5117 - val_acc: 0.7211\n",
      "Epoch 83/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5043 - acc: 0.7275\n",
      "Epoch 00083: val_loss did not improve from 0.50716\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.5043 - acc: 0.7275 - val_loss: 0.5135 - val_acc: 0.7211\n",
      "Epoch 84/100\n",
      "3360/3373 [============================>.] - ETA: 0s - loss: 0.5040 - acc: 0.7281\n",
      "Epoch 00084: val_loss did not improve from 0.50716\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.5039 - acc: 0.7281 - val_loss: 0.5144 - val_acc: 0.7211\n",
      "Epoch 85/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5028 - acc: 0.7286\n",
      "Epoch 00085: val_loss improved from 0.50716 to 0.50562, saving model to Step1_0_Epoch-85\n",
      "INFO:tensorflow:Assets written to: Step1_0_Epoch-85/assets\n",
      "3373/3373 [==============================] - 13s 4ms/step - loss: 0.5028 - acc: 0.7286 - val_loss: 0.5056 - val_acc: 0.7243\n",
      "Epoch 86/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5022 - acc: 0.7288\n",
      "Epoch 00086: val_loss improved from 0.50562 to 0.49171, saving model to Step1_0_Epoch-86\n",
      "INFO:tensorflow:Assets written to: Step1_0_Epoch-86/assets\n",
      "3373/3373 [==============================] - 13s 4ms/step - loss: 0.5021 - acc: 0.7288 - val_loss: 0.4917 - val_acc: 0.7342\n",
      "Epoch 87/100\n",
      "3359/3373 [============================>.] - ETA: 0s - loss: 0.5013 - acc: 0.7294\n",
      "Epoch 00087: val_loss did not improve from 0.49171\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.5013 - acc: 0.7294 - val_loss: 0.5143 - val_acc: 0.7192\n",
      "Epoch 88/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5010 - acc: 0.7301\n",
      "Epoch 00088: val_loss did not improve from 0.49171\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.5010 - acc: 0.7301 - val_loss: 0.5063 - val_acc: 0.7260\n",
      "Epoch 89/100\n",
      "3360/3373 [============================>.] - ETA: 0s - loss: 0.5000 - acc: 0.7303\n",
      "Epoch 00089: val_loss did not improve from 0.49171\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.5001 - acc: 0.7302 - val_loss: 0.5133 - val_acc: 0.7237\n",
      "Epoch 90/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.4997 - acc: 0.7310\n",
      "Epoch 00090: val_loss did not improve from 0.49171\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.4996 - acc: 0.7310 - val_loss: 0.4936 - val_acc: 0.7354\n",
      "Epoch 91/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.4991 - acc: 0.7309\n",
      "Epoch 00091: val_loss did not improve from 0.49171\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.4991 - acc: 0.7309 - val_loss: 0.5013 - val_acc: 0.7285\n",
      "Epoch 92/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.4970 - acc: 0.7326\n",
      "Epoch 00092: val_loss did not improve from 0.49171\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.4970 - acc: 0.7326 - val_loss: 0.4926 - val_acc: 0.7380\n",
      "Epoch 93/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.4969 - acc: 0.7323\n",
      "Epoch 00093: val_loss did not improve from 0.49171\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.4969 - acc: 0.7324 - val_loss: 0.4989 - val_acc: 0.7267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/100\n",
      "3360/3373 [============================>.] - ETA: 0s - loss: 0.4970 - acc: 0.7326\n",
      "Epoch 00094: val_loss did not improve from 0.49171\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.4970 - acc: 0.7326 - val_loss: 0.4979 - val_acc: 0.7303\n",
      "Epoch 95/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.4959 - acc: 0.7333\n",
      "Epoch 00095: val_loss did not improve from 0.49171\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.4959 - acc: 0.7333 - val_loss: 0.5064 - val_acc: 0.7290\n",
      "Epoch 96/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.4954 - acc: 0.7334\n",
      "Epoch 00096: val_loss improved from 0.49171 to 0.48818, saving model to Step1_0_Epoch-96\n",
      "INFO:tensorflow:Assets written to: Step1_0_Epoch-96/assets\n",
      "3373/3373 [==============================] - 13s 4ms/step - loss: 0.4954 - acc: 0.7334 - val_loss: 0.4882 - val_acc: 0.7368\n",
      "Epoch 97/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.4948 - acc: 0.7336\n",
      "Epoch 00097: val_loss did not improve from 0.48818\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.4948 - acc: 0.7336 - val_loss: 0.4985 - val_acc: 0.7291\n",
      "Epoch 98/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.4935 - acc: 0.7348\n",
      "Epoch 00098: val_loss did not improve from 0.48818\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.4935 - acc: 0.7348 - val_loss: 0.5026 - val_acc: 0.7290\n",
      "Epoch 99/100\n",
      "3360/3373 [============================>.] - ETA: 0s - loss: 0.4941 - acc: 0.7343\n",
      "Epoch 00099: val_loss did not improve from 0.48818\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.4941 - acc: 0.7343 - val_loss: 0.4993 - val_acc: 0.7312\n",
      "Epoch 100/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.4926 - acc: 0.7350\n",
      "Epoch 00100: val_loss improved from 0.48818 to 0.48742, saving model to Step1_0_Epoch-100\n",
      "INFO:tensorflow:Assets written to: Step1_0_Epoch-100/assets\n",
      "3373/3373 [==============================] - 13s 4ms/step - loss: 0.4926 - acc: 0.7350 - val_loss: 0.4874 - val_acc: 0.7379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vast/palmer/home.grace/ys668/analysis/run12/data/unfolding/toy/omnifold.py:32: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  w *= np.clip(preds/(1 - preds + 10**-50), fitargs.get('weight_clip_min', 0.), fitargs.get('weight_clip_max', np.inf))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3309/3323 [============================>.] - ETA: 0s - loss: 0.6992 - acc: 0.5004\n",
      "Epoch 00001: val_loss improved from inf to 0.72600, saving model to Step2_0_Epoch-1\n",
      "INFO:tensorflow:Assets written to: Step2_0_Epoch-1/assets\n",
      "3323/3323 [==============================] - 49s 15ms/step - loss: 0.6991 - acc: 0.5004 - val_loss: 0.7260 - val_acc: 0.5000\n",
      "Epoch 2/100\n",
      "3312/3323 [============================>.] - ETA: 0s - loss: 0.6993 - acc: 0.4999\n",
      "Epoch 00002: val_loss did not improve from 0.72600\n",
      "3323/3323 [==============================] - 12s 4ms/step - loss: 0.6991 - acc: 0.4999 - val_loss: 0.7420 - val_acc: 0.5000\n",
      "Epoch 3/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 0.6925 - acc: 0.5002\n",
      "Epoch 00003: val_loss did not improve from 0.72600\n",
      "3323/3323 [==============================] - 12s 4ms/step - loss: 0.6929 - acc: 0.5002 - val_loss: 0.7371 - val_acc: 0.5000\n",
      "Epoch 4/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 0.6973 - acc: 0.5003\n",
      "Epoch 00004: val_loss did not improve from 0.72600\n",
      "3323/3323 [==============================] - 12s 4ms/step - loss: 0.6972 - acc: 0.5003 - val_loss: 0.7334 - val_acc: 0.5000\n",
      "Epoch 5/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 0.6954 - acc: 0.4999\n",
      "Epoch 00005: val_loss did not improve from 0.72600\n",
      "3323/3323 [==============================] - 12s 4ms/step - loss: 0.6953 - acc: 0.4999 - val_loss: 0.7382 - val_acc: 0.5000\n",
      "Epoch 6/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 0.6987 - acc: 0.4999\n",
      "Epoch 00006: val_loss did not improve from 0.72600\n",
      "3323/3323 [==============================] - 12s 4ms/step - loss: 0.6987 - acc: 0.4999 - val_loss: 0.7411 - val_acc: 0.5000\n",
      "Epoch 7/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 0.6998 - acc: 0.4997\n",
      "Epoch 00007: val_loss did not improve from 0.72600\n",
      "3323/3323 [==============================] - 12s 4ms/step - loss: 0.6997 - acc: 0.4997 - val_loss: 0.7295 - val_acc: 0.5000\n",
      "Epoch 8/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 0.6965 - acc: 0.5000\n",
      "Epoch 00008: val_loss did not improve from 0.72600\n",
      "3323/3323 [==============================] - 12s 4ms/step - loss: 0.6965 - acc: 0.5001 - val_loss: 0.7354 - val_acc: 0.5000\n",
      "Epoch 9/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 0.7023 - acc: 0.5000\n",
      "Epoch 00009: val_loss did not improve from 0.72600\n",
      "3323/3323 [==============================] - 12s 4ms/step - loss: 0.7022 - acc: 0.5000 - val_loss: 0.7265 - val_acc: 0.5000\n",
      "Epoch 10/100\n",
      "3312/3323 [============================>.] - ETA: 0s - loss: 0.6924 - acc: 0.4999\n",
      "Epoch 00010: val_loss did not improve from 0.72600\n",
      "3323/3323 [==============================] - 12s 4ms/step - loss: 0.6923 - acc: 0.4999 - val_loss: 0.7276 - val_acc: 0.5000\n",
      "Epoch 11/100\n",
      "3312/3323 [============================>.] - ETA: 0s - loss: 0.6894 - acc: 0.4998\n",
      "Epoch 00011: val_loss did not improve from 0.72600\n",
      "Restoring model weights from the end of the best epoch.\n",
      "3323/3323 [==============================] - 12s 4ms/step - loss: 0.6892 - acc: 0.4998 - val_loss: 0.7279 - val_acc: 0.5000\n",
      "Epoch 00011: early stopping\n",
      "Model: \"functional_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 2)]               0         \n",
      "_________________________________________________________________\n",
      "dense_0 (Dense)              (None, 100)               300       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 2)                 202       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 20,702\n",
      "Trainable params: 20,702\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"functional_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 2)]               0         \n",
      "_________________________________________________________________\n",
      "dense_0 (Dense)              (None, 100)               300       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 2)                 202       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 20,702\n",
      "Trainable params: 20,702\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "3360/3373 [============================>.] - ETA: 0s - loss: 0.4687 - acc: 0.7353\n",
      "Epoch 00001: val_loss improved from inf to 0.47170, saving model to Step1_1_Epoch-1\n",
      "INFO:tensorflow:Assets written to: Step1_1_Epoch-1/assets\n",
      "3373/3373 [==============================] - 49s 15ms/step - loss: 0.4687 - acc: 0.7353 - val_loss: 0.4717 - val_acc: 0.7331\n",
      "Epoch 2/100\n",
      "3359/3373 [============================>.] - ETA: 0s - loss: 0.4692 - acc: 0.7352\n",
      "Epoch 00002: val_loss improved from 0.47170 to 0.46650, saving model to Step1_1_Epoch-2\n",
      "INFO:tensorflow:Assets written to: Step1_1_Epoch-2/assets\n",
      "3373/3373 [==============================] - 13s 4ms/step - loss: 0.4692 - acc: 0.7351 - val_loss: 0.4665 - val_acc: 0.7390\n",
      "Epoch 3/100\n",
      "3359/3373 [============================>.] - ETA: 0s - loss: 0.4686 - acc: 0.7353\n",
      "Epoch 00003: val_loss did not improve from 0.46650\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.4686 - acc: 0.7353 - val_loss: 0.4758 - val_acc: 0.7264\n",
      "Epoch 4/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.4684 - acc: 0.7353\n",
      "Epoch 00004: val_loss did not improve from 0.46650\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.4684 - acc: 0.7353 - val_loss: 0.4689 - val_acc: 0.7346\n",
      "Epoch 5/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.4679 - acc: 0.7357\n",
      "Epoch 00005: val_loss did not improve from 0.46650\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.4679 - acc: 0.7357 - val_loss: 0.4777 - val_acc: 0.7338\n",
      "Epoch 6/100\n",
      "3360/3373 [============================>.] - ETA: 0s - loss: 0.4668 - acc: 0.7364\n",
      "Epoch 00006: val_loss improved from 0.46650 to 0.46574, saving model to Step1_1_Epoch-6\n",
      "INFO:tensorflow:Assets written to: Step1_1_Epoch-6/assets\n",
      "3373/3373 [==============================] - 13s 4ms/step - loss: 0.4668 - acc: 0.7364 - val_loss: 0.4657 - val_acc: 0.7412\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3358/3373 [============================>.] - ETA: 0s - loss: 0.4665 - acc: 0.7361\n",
      "Epoch 00007: val_loss did not improve from 0.46574\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.4665 - acc: 0.7361 - val_loss: 0.4757 - val_acc: 0.7273\n",
      "Epoch 8/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.4661 - acc: 0.7367\n",
      "Epoch 00008: val_loss improved from 0.46574 to 0.46546, saving model to Step1_1_Epoch-8\n",
      "INFO:tensorflow:Assets written to: Step1_1_Epoch-8/assets\n",
      "3373/3373 [==============================] - 13s 4ms/step - loss: 0.4661 - acc: 0.7367 - val_loss: 0.4655 - val_acc: 0.7342\n",
      "Epoch 9/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.4656 - acc: 0.7371\n",
      "Epoch 00009: val_loss did not improve from 0.46546\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.4655 - acc: 0.7372 - val_loss: 0.4790 - val_acc: 0.7348\n",
      "Epoch 10/100\n",
      "3359/3373 [============================>.] - ETA: 0s - loss: 0.4652 - acc: 0.7368\n",
      "Epoch 00010: val_loss did not improve from 0.46546\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.4652 - acc: 0.7368 - val_loss: 0.4700 - val_acc: 0.7369\n",
      "Epoch 11/100\n",
      "3360/3373 [============================>.] - ETA: 0s - loss: 0.4646 - acc: 0.7378\n",
      "Epoch 00011: val_loss improved from 0.46546 to 0.46287, saving model to Step1_1_Epoch-11\n",
      "INFO:tensorflow:Assets written to: Step1_1_Epoch-11/assets\n",
      "3373/3373 [==============================] - 13s 4ms/step - loss: 0.4646 - acc: 0.7377 - val_loss: 0.4629 - val_acc: 0.7335\n",
      "Epoch 12/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.4645 - acc: 0.7378\n",
      "Epoch 00012: val_loss did not improve from 0.46287\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.4644 - acc: 0.7378 - val_loss: 0.4629 - val_acc: 0.7402\n",
      "Epoch 13/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.4636 - acc: 0.7389\n",
      "Epoch 00013: val_loss did not improve from 0.46287\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.4636 - acc: 0.7389 - val_loss: 0.4715 - val_acc: 0.7372\n",
      "Epoch 14/100\n",
      "3359/3373 [============================>.] - ETA: 0s - loss: 0.4628 - acc: 0.7391\n",
      "Epoch 00014: val_loss did not improve from 0.46287\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.4627 - acc: 0.7391 - val_loss: 0.4718 - val_acc: 0.7351\n",
      "Epoch 15/100\n",
      "3360/3373 [============================>.] - ETA: 0s - loss: 0.4627 - acc: 0.7389\n",
      "Epoch 00015: val_loss did not improve from 0.46287\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.4627 - acc: 0.7390 - val_loss: 0.4881 - val_acc: 0.7174\n",
      "Epoch 16/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.4621 - acc: 0.7398\n",
      "Epoch 00016: val_loss improved from 0.46287 to 0.45468, saving model to Step1_1_Epoch-16\n",
      "INFO:tensorflow:Assets written to: Step1_1_Epoch-16/assets\n",
      "3373/3373 [==============================] - 13s 4ms/step - loss: 0.4620 - acc: 0.7398 - val_loss: 0.4547 - val_acc: 0.7421\n",
      "Epoch 17/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.4622 - acc: 0.7397\n",
      "Epoch 00017: val_loss did not improve from 0.45468\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.4622 - acc: 0.7397 - val_loss: 0.4710 - val_acc: 0.7314\n",
      "Epoch 18/100\n",
      "3359/3373 [============================>.] - ETA: 0s - loss: 0.4609 - acc: 0.7405\n",
      "Epoch 00018: val_loss did not improve from 0.45468\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.4609 - acc: 0.7405 - val_loss: 0.4583 - val_acc: 0.7431\n",
      "Epoch 19/100\n",
      "3360/3373 [============================>.] - ETA: 0s - loss: 0.4609 - acc: 0.7401\n",
      "Epoch 00019: val_loss did not improve from 0.45468\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.4609 - acc: 0.7401 - val_loss: 0.4745 - val_acc: 0.7281\n",
      "Epoch 20/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.4605 - acc: 0.7407\n",
      "Epoch 00020: val_loss did not improve from 0.45468\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.4605 - acc: 0.7407 - val_loss: 0.4605 - val_acc: 0.7405\n",
      "Epoch 21/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.4596 - acc: 0.7414\n",
      "Epoch 00021: val_loss did not improve from 0.45468\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.4596 - acc: 0.7414 - val_loss: 0.4638 - val_acc: 0.7419\n",
      "Epoch 22/100\n",
      "3360/3373 [============================>.] - ETA: 0s - loss: 0.4591 - acc: 0.7418\n",
      "Epoch 00022: val_loss did not improve from 0.45468\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.4591 - acc: 0.7418 - val_loss: 0.4654 - val_acc: 0.7401\n",
      "Epoch 23/100\n",
      "3360/3373 [============================>.] - ETA: 0s - loss: 0.4592 - acc: 0.7418\n",
      "Epoch 00023: val_loss did not improve from 0.45468\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.4592 - acc: 0.7418 - val_loss: 0.4678 - val_acc: 0.7369\n",
      "Epoch 24/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.4585 - acc: 0.7420\n",
      "Epoch 00024: val_loss did not improve from 0.45468\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.4586 - acc: 0.7420 - val_loss: 0.4643 - val_acc: 0.7393\n",
      "Epoch 25/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.4580 - acc: 0.7426\n",
      "Epoch 00025: val_loss did not improve from 0.45468\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.4580 - acc: 0.7426 - val_loss: 0.4571 - val_acc: 0.7430\n",
      "Epoch 26/100\n",
      "3360/3373 [============================>.] - ETA: 0s - loss: 0.4583 - acc: 0.7423\n",
      "Epoch 00026: val_loss did not improve from 0.45468\n",
      "Restoring model weights from the end of the best epoch.\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.4582 - acc: 0.7423 - val_loss: 0.4664 - val_acc: 0.7340\n",
      "Epoch 00026: early stopping\n",
      "Epoch 1/100\n",
      "3312/3323 [============================>.] - ETA: 0s - loss: 0.6802 - acc: 0.4998\n",
      "Epoch 00001: val_loss improved from inf to 0.67726, saving model to Step2_1_Epoch-1\n",
      "INFO:tensorflow:Assets written to: Step2_1_Epoch-1/assets\n",
      "3323/3323 [==============================] - 48s 15ms/step - loss: 0.6801 - acc: 0.4998 - val_loss: 0.6773 - val_acc: 0.5000\n",
      "Epoch 2/100\n",
      "3312/3323 [============================>.] - ETA: 0s - loss: 0.6786 - acc: 0.4999\n",
      "Epoch 00002: val_loss did not improve from 0.67726\n",
      "3323/3323 [==============================] - 12s 4ms/step - loss: 0.6786 - acc: 0.5000 - val_loss: 0.6786 - val_acc: 0.5000\n",
      "Epoch 3/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 0.6793 - acc: 0.5002\n",
      "Epoch 00003: val_loss did not improve from 0.67726\n",
      "3323/3323 [==============================] - 12s 4ms/step - loss: 0.6792 - acc: 0.5002 - val_loss: 0.6789 - val_acc: 0.5000\n",
      "Epoch 4/100\n",
      "3312/3323 [============================>.] - ETA: 0s - loss: 0.6790 - acc: 0.5000\n",
      "Epoch 00004: val_loss did not improve from 0.67726\n",
      "3323/3323 [==============================] - 12s 4ms/step - loss: 0.6789 - acc: 0.5000 - val_loss: 0.6773 - val_acc: 0.5000\n",
      "Epoch 5/100\n",
      "3312/3323 [============================>.] - ETA: 0s - loss: 0.6798 - acc: 0.4996\n",
      "Epoch 00005: val_loss did not improve from 0.67726\n",
      "3323/3323 [==============================] - 12s 3ms/step - loss: 0.6797 - acc: 0.4996 - val_loss: 0.6779 - val_acc: 0.5000\n",
      "Epoch 6/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 0.6795 - acc: 0.4999\n",
      "Epoch 00006: val_loss did not improve from 0.67726\n",
      "3323/3323 [==============================] - 12s 3ms/step - loss: 0.6794 - acc: 0.4999 - val_loss: 0.6774 - val_acc: 0.5000\n",
      "Epoch 7/100\n",
      "3317/3323 [============================>.] - ETA: 0s - loss: 0.6790 - acc: 0.5003\n",
      "Epoch 00007: val_loss improved from 0.67726 to 0.67665, saving model to Step2_1_Epoch-7\n",
      "INFO:tensorflow:Assets written to: Step2_1_Epoch-7/assets\n",
      "3323/3323 [==============================] - 12s 4ms/step - loss: 0.6790 - acc: 0.5003 - val_loss: 0.6766 - val_acc: 0.5000\n",
      "Epoch 8/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 0.6788 - acc: 0.4998\n",
      "Epoch 00008: val_loss did not improve from 0.67665\n",
      "3323/3323 [==============================] - 12s 4ms/step - loss: 0.6787 - acc: 0.4998 - val_loss: 0.6778 - val_acc: 0.5000\n",
      "Epoch 9/100\n",
      "3311/3323 [============================>.] - ETA: 0s - loss: 0.6787 - acc: 0.5003\n",
      "Epoch 00009: val_loss did not improve from 0.67665\n",
      "3323/3323 [==============================] - 12s 4ms/step - loss: 0.6786 - acc: 0.5003 - val_loss: 0.6795 - val_acc: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100\n",
      "3312/3323 [============================>.] - ETA: 0s - loss: 0.6788 - acc: 0.5000\n",
      "Epoch 00010: val_loss did not improve from 0.67665\n",
      "3323/3323 [==============================] - 12s 4ms/step - loss: 0.6786 - acc: 0.5000 - val_loss: 0.6809 - val_acc: 0.5000\n",
      "Epoch 11/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 0.6800 - acc: 0.5000\n",
      "Epoch 00011: val_loss did not improve from 0.67665\n",
      "3323/3323 [==============================] - 12s 4ms/step - loss: 0.6800 - acc: 0.4999 - val_loss: 0.6781 - val_acc: 0.5000\n",
      "Epoch 12/100\n",
      "3311/3323 [============================>.] - ETA: 0s - loss: 0.6778 - acc: 0.5004\n",
      "Epoch 00012: val_loss did not improve from 0.67665\n",
      "3323/3323 [==============================] - 12s 4ms/step - loss: 0.6778 - acc: 0.5003 - val_loss: 0.6818 - val_acc: 0.5000\n",
      "Epoch 13/100\n",
      "3312/3323 [============================>.] - ETA: 0s - loss: 0.6805 - acc: 0.4995\n",
      "Epoch 00013: val_loss did not improve from 0.67665\n",
      "3323/3323 [==============================] - 12s 4ms/step - loss: 0.6804 - acc: 0.4996 - val_loss: 0.6775 - val_acc: 0.5000\n",
      "Epoch 14/100\n",
      "3312/3323 [============================>.] - ETA: 0s - loss: 0.6792 - acc: 0.5000\n",
      "Epoch 00014: val_loss did not improve from 0.67665\n",
      "3323/3323 [==============================] - 12s 4ms/step - loss: 0.6791 - acc: 0.5000 - val_loss: 0.6781 - val_acc: 0.5000\n",
      "Epoch 15/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 0.6787 - acc: 0.4999\n",
      "Epoch 00015: val_loss did not improve from 0.67665\n",
      "3323/3323 [==============================] - 12s 4ms/step - loss: 0.6786 - acc: 0.4999 - val_loss: 0.6780 - val_acc: 0.5000\n",
      "Epoch 16/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 0.6794 - acc: 0.4997\n",
      "Epoch 00016: val_loss did not improve from 0.67665\n",
      "3323/3323 [==============================] - 12s 4ms/step - loss: 0.6793 - acc: 0.4998 - val_loss: 0.6774 - val_acc: 0.5000\n",
      "Epoch 17/100\n",
      "3311/3323 [============================>.] - ETA: 0s - loss: 0.6789 - acc: 0.4997\n",
      "Epoch 00017: val_loss did not improve from 0.67665\n",
      "Restoring model weights from the end of the best epoch.\n",
      "3323/3323 [==============================] - 12s 4ms/step - loss: 0.6789 - acc: 0.4997 - val_loss: 0.6771 - val_acc: 0.5000\n",
      "Epoch 00017: early stopping\n",
      "Model: \"functional_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 2)]               0         \n",
      "_________________________________________________________________\n",
      "dense_0 (Dense)              (None, 100)               300       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 2)                 202       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 20,702\n",
      "Trainable params: 20,702\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"functional_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 2)]               0         \n",
      "_________________________________________________________________\n",
      "dense_0 (Dense)              (None, 100)               300       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 2)                 202       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 20,702\n",
      "Trainable params: 20,702\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "3360/3373 [============================>.] - ETA: 0s - loss: 0.4792 - acc: 0.7402\n",
      "Epoch 00001: val_loss improved from inf to 0.48176, saving model to Step1_2_Epoch-1\n",
      "INFO:tensorflow:Assets written to: Step1_2_Epoch-1/assets\n",
      "3373/3373 [==============================] - 49s 14ms/step - loss: 0.4792 - acc: 0.7402 - val_loss: 0.4818 - val_acc: 0.7372\n",
      "Epoch 2/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.4778 - acc: 0.7409\n",
      "Epoch 00002: val_loss improved from 0.48176 to 0.47612, saving model to Step1_2_Epoch-2\n",
      "INFO:tensorflow:Assets written to: Step1_2_Epoch-2/assets\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.4778 - acc: 0.7409 - val_loss: 0.4761 - val_acc: 0.7391\n",
      "Epoch 3/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.4777 - acc: 0.7413\n",
      "Epoch 00003: val_loss improved from 0.47612 to 0.47476, saving model to Step1_2_Epoch-3\n",
      "INFO:tensorflow:Assets written to: Step1_2_Epoch-3/assets\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.4777 - acc: 0.7413 - val_loss: 0.4748 - val_acc: 0.7394\n",
      "Epoch 4/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.4764 - acc: 0.7419\n",
      "Epoch 00004: val_loss did not improve from 0.47476\n",
      "3373/3373 [==============================] - 12s 3ms/step - loss: 0.4764 - acc: 0.7418 - val_loss: 0.4907 - val_acc: 0.7360\n",
      "Epoch 5/100\n",
      "3360/3373 [============================>.] - ETA: 0s - loss: 0.4757 - acc: 0.7422\n",
      "Epoch 00005: val_loss did not improve from 0.47476\n",
      "3373/3373 [==============================] - 12s 3ms/step - loss: 0.4758 - acc: 0.7421 - val_loss: 0.4812 - val_acc: 0.7379\n",
      "Epoch 6/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.4758 - acc: 0.7424\n",
      "Epoch 00006: val_loss did not improve from 0.47476\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.4758 - acc: 0.7423 - val_loss: 0.4754 - val_acc: 0.7410\n",
      "Epoch 7/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.4754 - acc: 0.7425\n",
      "Epoch 00007: val_loss did not improve from 0.47476\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.4754 - acc: 0.7425 - val_loss: 0.4811 - val_acc: 0.7358\n",
      "Epoch 8/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.4745 - acc: 0.7430\n",
      "Epoch 00008: val_loss did not improve from 0.47476\n",
      "3373/3373 [==============================] - 12s 3ms/step - loss: 0.4744 - acc: 0.7430 - val_loss: 0.4821 - val_acc: 0.7342\n",
      "Epoch 9/100\n",
      "3360/3373 [============================>.] - ETA: 0s - loss: 0.4744 - acc: 0.7429\n",
      "Epoch 00009: val_loss improved from 0.47476 to 0.47284, saving model to Step1_2_Epoch-9\n",
      "INFO:tensorflow:Assets written to: Step1_2_Epoch-9/assets\n",
      "3373/3373 [==============================] - 13s 4ms/step - loss: 0.4744 - acc: 0.7429 - val_loss: 0.4728 - val_acc: 0.7446\n",
      "Epoch 10/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3366/3373 [============================>.] - ETA: 0s - loss: 0.4739 - acc: 0.7434\n",
      "Epoch 00010: val_loss did not improve from 0.47284\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.4739 - acc: 0.7434 - val_loss: 0.4748 - val_acc: 0.7420\n",
      "Epoch 11/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.4733 - acc: 0.7437\n",
      "Epoch 00011: val_loss improved from 0.47284 to 0.46649, saving model to Step1_2_Epoch-11\n",
      "INFO:tensorflow:Assets written to: Step1_2_Epoch-11/assets\n",
      "3373/3373 [==============================] - 13s 4ms/step - loss: 0.4733 - acc: 0.7437 - val_loss: 0.4665 - val_acc: 0.7455\n",
      "Epoch 12/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.4727 - acc: 0.7444\n",
      "Epoch 00012: val_loss did not improve from 0.46649\n",
      "3373/3373 [==============================] - 12s 3ms/step - loss: 0.4728 - acc: 0.7444 - val_loss: 0.4841 - val_acc: 0.7384\n",
      "Epoch 13/100\n",
      "3360/3373 [============================>.] - ETA: 0s - loss: 0.4724 - acc: 0.7437\n",
      "Epoch 00013: val_loss did not improve from 0.46649\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.4724 - acc: 0.7437 - val_loss: 0.4707 - val_acc: 0.7481\n",
      "Epoch 14/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.4715 - acc: 0.7446\n",
      "Epoch 00014: val_loss did not improve from 0.46649\n",
      "3373/3373 [==============================] - 12s 3ms/step - loss: 0.4715 - acc: 0.7447 - val_loss: 0.4773 - val_acc: 0.7429\n",
      "Epoch 15/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.4720 - acc: 0.7445\n",
      "Epoch 00015: val_loss improved from 0.46649 to 0.46203, saving model to Step1_2_Epoch-15\n",
      "INFO:tensorflow:Assets written to: Step1_2_Epoch-15/assets\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.4720 - acc: 0.7445 - val_loss: 0.4620 - val_acc: 0.7527\n",
      "Epoch 16/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.4712 - acc: 0.7450\n",
      "Epoch 00016: val_loss did not improve from 0.46203\n",
      "3373/3373 [==============================] - 12s 3ms/step - loss: 0.4712 - acc: 0.7451 - val_loss: 0.4754 - val_acc: 0.7439\n",
      "Epoch 17/100\n",
      "3365/3373 [============================>.] - ETA: 0s - loss: 0.4706 - acc: 0.7451\n",
      "Epoch 00017: val_loss improved from 0.46203 to 0.45875, saving model to Step1_2_Epoch-17\n",
      "INFO:tensorflow:Assets written to: Step1_2_Epoch-17/assets\n",
      "3373/3373 [==============================] - 13s 4ms/step - loss: 0.4706 - acc: 0.7451 - val_loss: 0.4587 - val_acc: 0.7546\n",
      "Epoch 18/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.4708 - acc: 0.7452\n",
      "Epoch 00018: val_loss did not improve from 0.45875\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.4709 - acc: 0.7452 - val_loss: 0.4765 - val_acc: 0.7431\n",
      "Epoch 19/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.4701 - acc: 0.7456\n",
      "Epoch 00019: val_loss did not improve from 0.45875\n",
      "3373/3373 [==============================] - 12s 3ms/step - loss: 0.4700 - acc: 0.7456 - val_loss: 0.4821 - val_acc: 0.7391\n",
      "Epoch 20/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.4693 - acc: 0.7463\n",
      "Epoch 00020: val_loss did not improve from 0.45875\n",
      "3373/3373 [==============================] - 12s 3ms/step - loss: 0.4693 - acc: 0.7463 - val_loss: 0.4827 - val_acc: 0.7353\n",
      "Epoch 21/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.4694 - acc: 0.7462\n",
      "Epoch 00021: val_loss did not improve from 0.45875\n",
      "3373/3373 [==============================] - 12s 3ms/step - loss: 0.4694 - acc: 0.7463 - val_loss: 0.4729 - val_acc: 0.7389\n",
      "Epoch 22/100\n",
      "3360/3373 [============================>.] - ETA: 0s - loss: 0.4688 - acc: 0.7464\n",
      "Epoch 00022: val_loss did not improve from 0.45875\n",
      "3373/3373 [==============================] - 12s 3ms/step - loss: 0.4688 - acc: 0.7464 - val_loss: 0.4638 - val_acc: 0.7507\n",
      "Epoch 23/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.4683 - acc: 0.7474\n",
      "Epoch 00023: val_loss did not improve from 0.45875\n",
      "3373/3373 [==============================] - 12s 3ms/step - loss: 0.4683 - acc: 0.7474 - val_loss: 0.4737 - val_acc: 0.7398\n",
      "Epoch 24/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.4685 - acc: 0.7468\n",
      "Epoch 00024: val_loss did not improve from 0.45875\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.4685 - acc: 0.7468 - val_loss: 0.4737 - val_acc: 0.7461\n",
      "Epoch 25/100\n",
      "3363/3373 [============================>.] - ETA: 0s - loss: 0.4676 - acc: 0.7477\n",
      "Epoch 00025: val_loss did not improve from 0.45875\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.4677 - acc: 0.7477 - val_loss: 0.4692 - val_acc: 0.7451\n",
      "Epoch 26/100\n",
      "3358/3373 [============================>.] - ETA: 0s - loss: 0.4668 - acc: 0.7480\n",
      "Epoch 00026: val_loss did not improve from 0.45875\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.4668 - acc: 0.7480 - val_loss: 0.4684 - val_acc: 0.7465\n",
      "Epoch 27/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.4671 - acc: 0.7477\n",
      "Epoch 00027: val_loss did not improve from 0.45875\n",
      "Restoring model weights from the end of the best epoch.\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.4671 - acc: 0.7477 - val_loss: 0.4865 - val_acc: 0.7339\n",
      "Epoch 00027: early stopping\n",
      "Epoch 1/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 0.9012 - acc: 0.4997\n",
      "Epoch 00001: val_loss improved from inf to 0.85006, saving model to Step2_2_Epoch-1\n",
      "INFO:tensorflow:Assets written to: Step2_2_Epoch-1/assets\n",
      "3323/3323 [==============================] - 48s 14ms/step - loss: 0.9009 - acc: 0.4997 - val_loss: 0.8501 - val_acc: 0.5000\n",
      "Epoch 2/100\n",
      "3312/3323 [============================>.] - ETA: 0s - loss: 0.8235 - acc: 0.5000\n",
      "Epoch 00002: val_loss did not improve from 0.85006\n",
      "3323/3323 [==============================] - 12s 3ms/step - loss: 0.8233 - acc: 0.5000 - val_loss: 0.8700 - val_acc: 0.5000\n",
      "Epoch 3/100\n",
      "3318/3323 [============================>.] - ETA: 0s - loss: 0.8168 - acc: 0.5001\n",
      "Epoch 00003: val_loss improved from 0.85006 to 0.83639, saving model to Step2_2_Epoch-3\n",
      "INFO:tensorflow:Assets written to: Step2_2_Epoch-3/assets\n",
      "3323/3323 [==============================] - 12s 4ms/step - loss: 0.8166 - acc: 0.5001 - val_loss: 0.8364 - val_acc: 0.5000\n",
      "Epoch 4/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 0.8018 - acc: 0.5001\n",
      "Epoch 00004: val_loss improved from 0.83639 to 0.83477, saving model to Step2_2_Epoch-4\n",
      "INFO:tensorflow:Assets written to: Step2_2_Epoch-4/assets\n",
      "3323/3323 [==============================] - 12s 4ms/step - loss: 0.8016 - acc: 0.5001 - val_loss: 0.8348 - val_acc: 0.5000\n",
      "Epoch 5/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 0.8270 - acc: 0.5007\n",
      "Epoch 00005: val_loss did not improve from 0.83477\n",
      "3323/3323 [==============================] - 12s 3ms/step - loss: 0.8269 - acc: 0.5007 - val_loss: 0.8486 - val_acc: 0.5000\n",
      "Epoch 6/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 0.8002 - acc: 0.5001\n",
      "Epoch 00006: val_loss did not improve from 0.83477\n",
      "3323/3323 [==============================] - 12s 3ms/step - loss: 0.7998 - acc: 0.5001 - val_loss: 0.8381 - val_acc: 0.5000\n",
      "Epoch 7/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 0.8122 - acc: 0.4998\n",
      "Epoch 00007: val_loss improved from 0.83477 to 0.82437, saving model to Step2_2_Epoch-7\n",
      "INFO:tensorflow:Assets written to: Step2_2_Epoch-7/assets\n",
      "3323/3323 [==============================] - 12s 4ms/step - loss: 0.8119 - acc: 0.4999 - val_loss: 0.8244 - val_acc: 0.5000\n",
      "Epoch 8/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 0.8008 - acc: 0.4997\n",
      "Epoch 00008: val_loss did not improve from 0.82437\n",
      "3323/3323 [==============================] - 12s 3ms/step - loss: 0.8005 - acc: 0.4997 - val_loss: 0.8548 - val_acc: 0.5000\n",
      "Epoch 9/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 0.7967 - acc: 0.5000\n",
      "Epoch 00009: val_loss improved from 0.82437 to 0.80590, saving model to Step2_2_Epoch-9\n",
      "INFO:tensorflow:Assets written to: Step2_2_Epoch-9/assets\n",
      "3323/3323 [==============================] - 12s 4ms/step - loss: 0.7965 - acc: 0.5000 - val_loss: 0.8059 - val_acc: 0.5000\n",
      "Epoch 10/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3312/3323 [============================>.] - ETA: 0s - loss: 0.8018 - acc: 0.5001\n",
      "Epoch 00010: val_loss did not improve from 0.80590\n",
      "3323/3323 [==============================] - 12s 3ms/step - loss: 0.8017 - acc: 0.5001 - val_loss: 0.8752 - val_acc: 0.5000\n",
      "Epoch 11/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 0.8020 - acc: 0.5001\n",
      "Epoch 00011: val_loss did not improve from 0.80590\n",
      "3323/3323 [==============================] - 12s 3ms/step - loss: 0.8018 - acc: 0.5001 - val_loss: 0.8781 - val_acc: 0.5000\n",
      "Epoch 12/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 0.7993 - acc: 0.5001\n",
      "Epoch 00012: val_loss did not improve from 0.80590\n",
      "3323/3323 [==============================] - 12s 3ms/step - loss: 0.7990 - acc: 0.5001 - val_loss: 0.8159 - val_acc: 0.5000\n",
      "Epoch 13/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 0.7704 - acc: 0.4998\n",
      "Epoch 00013: val_loss did not improve from 0.80590\n",
      "3323/3323 [==============================] - 12s 4ms/step - loss: 0.7704 - acc: 0.4998 - val_loss: 0.8150 - val_acc: 0.5000\n",
      "Epoch 14/100\n",
      "3312/3323 [============================>.] - ETA: 0s - loss: 0.8052 - acc: 0.4999\n",
      "Epoch 00014: val_loss did not improve from 0.80590\n",
      "3323/3323 [==============================] - 12s 4ms/step - loss: 0.8049 - acc: 0.4999 - val_loss: 0.8431 - val_acc: 0.5000\n",
      "Epoch 15/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 0.7818 - acc: 0.5000\n",
      "Epoch 00015: val_loss did not improve from 0.80590\n",
      "3323/3323 [==============================] - 12s 3ms/step - loss: 0.7819 - acc: 0.4999 - val_loss: 0.8154 - val_acc: 0.5000\n",
      "Epoch 16/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 0.7675 - acc: 0.5001\n",
      "Epoch 00016: val_loss improved from 0.80590 to 0.78771, saving model to Step2_2_Epoch-16\n",
      "INFO:tensorflow:Assets written to: Step2_2_Epoch-16/assets\n",
      "3323/3323 [==============================] - 12s 4ms/step - loss: 0.7674 - acc: 0.5001 - val_loss: 0.7877 - val_acc: 0.5000\n",
      "Epoch 17/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 0.7795 - acc: 0.5003\n",
      "Epoch 00017: val_loss did not improve from 0.78771\n",
      "3323/3323 [==============================] - 12s 3ms/step - loss: 0.7796 - acc: 0.5002 - val_loss: 0.8679 - val_acc: 0.5000\n",
      "Epoch 18/100\n",
      "3317/3323 [============================>.] - ETA: 0s - loss: 0.8453 - acc: 0.5001\n",
      "Epoch 00018: val_loss did not improve from 0.78771\n",
      "3323/3323 [==============================] - 12s 4ms/step - loss: 0.8450 - acc: 0.5000 - val_loss: 0.8271 - val_acc: 0.5000\n",
      "Epoch 19/100\n",
      "3312/3323 [============================>.] - ETA: 0s - loss: 0.7707 - acc: 0.5002\n",
      "Epoch 00019: val_loss did not improve from 0.78771\n",
      "3323/3323 [==============================] - 12s 3ms/step - loss: 0.7704 - acc: 0.5002 - val_loss: 0.7936 - val_acc: 0.5000\n",
      "Epoch 20/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 0.7757 - acc: 0.4999\n",
      "Epoch 00020: val_loss did not improve from 0.78771\n",
      "3323/3323 [==============================] - 12s 3ms/step - loss: 0.7759 - acc: 0.4999 - val_loss: 0.9022 - val_acc: 0.5000\n",
      "Epoch 21/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 0.7913 - acc: 0.5002\n",
      "Epoch 00021: val_loss did not improve from 0.78771\n",
      "3323/3323 [==============================] - 12s 3ms/step - loss: 0.7912 - acc: 0.5002 - val_loss: 0.8347 - val_acc: 0.5000\n",
      "Epoch 22/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 0.7715 - acc: 0.4998\n",
      "Epoch 00022: val_loss did not improve from 0.78771\n",
      "3323/3323 [==============================] - 12s 3ms/step - loss: 0.7712 - acc: 0.4998 - val_loss: 0.8805 - val_acc: 0.5000\n",
      "Epoch 23/100\n",
      "3312/3323 [============================>.] - ETA: 0s - loss: 0.7741 - acc: 0.4996\n",
      "Epoch 00023: val_loss did not improve from 0.78771\n",
      "3323/3323 [==============================] - 12s 3ms/step - loss: 0.7739 - acc: 0.4996 - val_loss: 0.8263 - val_acc: 0.5000\n",
      "Epoch 24/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 0.7582 - acc: 0.5005\n",
      "Epoch 00024: val_loss did not improve from 0.78771\n",
      "3323/3323 [==============================] - 12s 3ms/step - loss: 0.7585 - acc: 0.5005 - val_loss: 0.8997 - val_acc: 0.5000\n",
      "Epoch 25/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 0.8470 - acc: 0.5003\n",
      "Epoch 00025: val_loss did not improve from 0.78771\n",
      "3323/3323 [==============================] - 12s 3ms/step - loss: 0.8469 - acc: 0.5002 - val_loss: 0.9170 - val_acc: 0.5000\n",
      "Epoch 26/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 0.8128 - acc: 0.4998\n",
      "Epoch 00026: val_loss improved from 0.78771 to 0.77127, saving model to Step2_2_Epoch-26\n",
      "INFO:tensorflow:Assets written to: Step2_2_Epoch-26/assets\n",
      "3323/3323 [==============================] - 12s 4ms/step - loss: 0.8125 - acc: 0.4998 - val_loss: 0.7713 - val_acc: 0.5000\n",
      "Epoch 27/100\n",
      "3312/3323 [============================>.] - ETA: 0s - loss: 0.8195 - acc: 0.4999\n",
      "Epoch 00027: val_loss did not improve from 0.77127\n",
      "3323/3323 [==============================] - 12s 3ms/step - loss: 0.8192 - acc: 0.4999 - val_loss: 0.8262 - val_acc: 0.5000\n",
      "Epoch 28/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 0.8662 - acc: 0.4998\n",
      "Epoch 00028: val_loss did not improve from 0.77127\n",
      "3323/3323 [==============================] - 12s 3ms/step - loss: 0.8660 - acc: 0.4997 - val_loss: 0.8604 - val_acc: 0.5000\n",
      "Epoch 29/100\n",
      "3318/3323 [============================>.] - ETA: 0s - loss: 0.8339 - acc: 0.4994\n",
      "Epoch 00029: val_loss did not improve from 0.77127\n",
      "3323/3323 [==============================] - 12s 4ms/step - loss: 0.8338 - acc: 0.4994 - val_loss: 0.8868 - val_acc: 0.5000\n",
      "Epoch 30/100\n",
      "3311/3323 [============================>.] - ETA: 0s - loss: 0.9301 - acc: 0.5002\n",
      "Epoch 00030: val_loss did not improve from 0.77127\n",
      "3323/3323 [==============================] - 12s 3ms/step - loss: 0.9298 - acc: 0.5002 - val_loss: 0.8363 - val_acc: 0.5000\n",
      "Epoch 31/100\n",
      "3312/3323 [============================>.] - ETA: 0s - loss: 0.8062 - acc: 0.5000\n",
      "Epoch 00031: val_loss did not improve from 0.77127\n",
      "3323/3323 [==============================] - 12s 3ms/step - loss: 0.8060 - acc: 0.5000 - val_loss: 0.8035 - val_acc: 0.5000\n",
      "Epoch 32/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 0.8121 - acc: 0.4999\n",
      "Epoch 00032: val_loss did not improve from 0.77127\n",
      "3323/3323 [==============================] - 12s 3ms/step - loss: 0.8123 - acc: 0.4999 - val_loss: 0.8263 - val_acc: 0.5000\n",
      "Epoch 33/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 0.8342 - acc: 0.5000\n",
      "Epoch 00033: val_loss did not improve from 0.77127\n",
      "3323/3323 [==============================] - 12s 4ms/step - loss: 0.8339 - acc: 0.5000 - val_loss: 0.8025 - val_acc: 0.5000\n",
      "Epoch 34/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 0.7719 - acc: 0.4998\n",
      "Epoch 00034: val_loss did not improve from 0.77127\n",
      "3323/3323 [==============================] - 12s 3ms/step - loss: 0.7719 - acc: 0.4998 - val_loss: 0.7858 - val_acc: 0.5000\n",
      "Epoch 35/100\n",
      "3312/3323 [============================>.] - ETA: 0s - loss: 0.7959 - acc: 0.4999\n",
      "Epoch 00035: val_loss did not improve from 0.77127\n",
      "3323/3323 [==============================] - 12s 3ms/step - loss: 0.7959 - acc: 0.4999 - val_loss: 0.8450 - val_acc: 0.5000\n",
      "Epoch 36/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 0.7648 - acc: 0.5002\n",
      "Epoch 00036: val_loss did not improve from 0.77127\n",
      "Restoring model weights from the end of the best epoch.\n",
      "3323/3323 [==============================] - 12s 3ms/step - loss: 0.7648 - acc: 0.5002 - val_loss: 0.7742 - val_acc: 0.5000\n",
      "Epoch 00036: early stopping\n",
      "Model: \"functional_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 2)]               0         \n",
      "_________________________________________________________________\n",
      "dense_0 (Dense)              (None, 100)               300       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 2)                 202       \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 20,702\n",
      "Trainable params: 20,702\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"functional_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 2)]               0         \n",
      "_________________________________________________________________\n",
      "dense_0 (Dense)              (None, 100)               300       \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 2)                 202       \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 20,702\n",
      "Trainable params: 20,702\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3360/3373 [============================>.] - ETA: 0s - loss: 0.5955 - acc: 0.7125\n",
      "Epoch 00001: val_loss improved from inf to 0.72856, saving model to Step1_3_Epoch-1\n",
      "INFO:tensorflow:Assets written to: Step1_3_Epoch-1/assets\n",
      "3373/3373 [==============================] - 49s 14ms/step - loss: 0.5955 - acc: 0.7125 - val_loss: 0.7286 - val_acc: 0.7001\n",
      "Epoch 2/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5527 - acc: 0.7195\n",
      "Epoch 00002: val_loss improved from 0.72856 to 0.53025, saving model to Step1_3_Epoch-2\n",
      "INFO:tensorflow:Assets written to: Step1_3_Epoch-2/assets\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.5526 - acc: 0.7195 - val_loss: 0.5303 - val_acc: 0.7282\n",
      "Epoch 3/100\n",
      "3360/3373 [============================>.] - ETA: 0s - loss: 0.5444 - acc: 0.7210\n",
      "Epoch 00003: val_loss did not improve from 0.53025\n",
      "3373/3373 [==============================] - 12s 3ms/step - loss: 0.5445 - acc: 0.7210 - val_loss: 0.5556 - val_acc: 0.7202\n",
      "Epoch 4/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5533 - acc: 0.7204\n",
      "Epoch 00004: val_loss improved from 0.53025 to 0.52902, saving model to Step1_3_Epoch-4\n",
      "INFO:tensorflow:Assets written to: Step1_3_Epoch-4/assets\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.5532 - acc: 0.7204 - val_loss: 0.5290 - val_acc: 0.7203\n",
      "Epoch 5/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5484 - acc: 0.7209\n",
      "Epoch 00005: val_loss did not improve from 0.52902\n",
      "3373/3373 [==============================] - 12s 3ms/step - loss: 0.5483 - acc: 0.7209 - val_loss: 0.5401 - val_acc: 0.7078\n",
      "Epoch 6/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5418 - acc: 0.7223\n",
      "Epoch 00006: val_loss improved from 0.52902 to 0.52452, saving model to Step1_3_Epoch-6\n",
      "INFO:tensorflow:Assets written to: Step1_3_Epoch-6/assets\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.5417 - acc: 0.7224 - val_loss: 0.5245 - val_acc: 0.7252\n",
      "Epoch 7/100\n",
      "3367/3373 [============================>.] - ETA: 0s - loss: 0.5497 - acc: 0.7218\n",
      "Epoch 00007: val_loss did not improve from 0.52452\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.5497 - acc: 0.7218 - val_loss: 0.5341 - val_acc: 0.7239\n",
      "Epoch 8/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5451 - acc: 0.7234\n",
      "Epoch 00008: val_loss did not improve from 0.52452\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.5452 - acc: 0.7234 - val_loss: 0.5605 - val_acc: 0.7144\n",
      "Epoch 9/100\n",
      "3360/3373 [============================>.] - ETA: 0s - loss: 0.5484 - acc: 0.7222\n",
      "Epoch 00009: val_loss improved from 0.52452 to 0.52309, saving model to Step1_3_Epoch-9\n",
      "INFO:tensorflow:Assets written to: Step1_3_Epoch-9/assets\n",
      "3373/3373 [==============================] - 13s 4ms/step - loss: 0.5483 - acc: 0.7222 - val_loss: 0.5231 - val_acc: 0.7312\n",
      "Epoch 10/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5352 - acc: 0.7247\n",
      "Epoch 00010: val_loss improved from 0.52309 to 0.51596, saving model to Step1_3_Epoch-10\n",
      "INFO:tensorflow:Assets written to: Step1_3_Epoch-10/assets\n",
      "3373/3373 [==============================] - 13s 4ms/step - loss: 0.5352 - acc: 0.7247 - val_loss: 0.5160 - val_acc: 0.7288\n",
      "Epoch 11/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5412 - acc: 0.7236\n",
      "Epoch 00011: val_loss did not improve from 0.51596\n",
      "3373/3373 [==============================] - 12s 3ms/step - loss: 0.5411 - acc: 0.7236 - val_loss: 0.5340 - val_acc: 0.7149\n",
      "Epoch 12/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5275 - acc: 0.7259\n",
      "Epoch 00012: val_loss did not improve from 0.51596\n",
      "3373/3373 [==============================] - 12s 3ms/step - loss: 0.5277 - acc: 0.7259 - val_loss: 0.5548 - val_acc: 0.7217\n",
      "Epoch 13/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5392 - acc: 0.7237\n",
      "Epoch 00013: val_loss did not improve from 0.51596\n",
      "3373/3373 [==============================] - 12s 3ms/step - loss: 0.5392 - acc: 0.7237 - val_loss: 0.5293 - val_acc: 0.7281\n",
      "Epoch 14/100\n",
      "3360/3373 [============================>.] - ETA: 0s - loss: 0.5371 - acc: 0.7249\n",
      "Epoch 00014: val_loss did not improve from 0.51596\n",
      "3373/3373 [==============================] - 12s 3ms/step - loss: 0.5370 - acc: 0.7249 - val_loss: 0.5350 - val_acc: 0.7264\n",
      "Epoch 15/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5374 - acc: 0.7250\n",
      "Epoch 00015: val_loss improved from 0.51596 to 0.50788, saving model to Step1_3_Epoch-15\n",
      "INFO:tensorflow:Assets written to: Step1_3_Epoch-15/assets\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.5373 - acc: 0.7250 - val_loss: 0.5079 - val_acc: 0.7356\n",
      "Epoch 16/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5358 - acc: 0.7255\n",
      "Epoch 00016: val_loss did not improve from 0.50788\n",
      "3373/3373 [==============================] - 12s 3ms/step - loss: 0.5359 - acc: 0.7255 - val_loss: 0.5630 - val_acc: 0.7130\n",
      "Epoch 17/100\n",
      "3363/3373 [============================>.] - ETA: 0s - loss: 0.5270 - acc: 0.7278\n",
      "Epoch 00017: val_loss did not improve from 0.50788\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.5269 - acc: 0.7278 - val_loss: 0.5080 - val_acc: 0.7290\n",
      "Epoch 18/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5295 - acc: 0.7266\n",
      "Epoch 00018: val_loss did not improve from 0.50788\n",
      "3373/3373 [==============================] - 12s 3ms/step - loss: 0.5297 - acc: 0.7266 - val_loss: 0.6346 - val_acc: 0.7021\n",
      "Epoch 19/100\n",
      "3360/3373 [============================>.] - ETA: 0s - loss: 0.5336 - acc: 0.7268\n",
      "Epoch 00019: val_loss did not improve from 0.50788\n",
      "3373/3373 [==============================] - 12s 3ms/step - loss: 0.5344 - acc: 0.7267 - val_loss: 0.7328 - val_acc: 0.6689\n",
      "Epoch 20/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5382 - acc: 0.7258\n",
      "Epoch 00020: val_loss did not improve from 0.50788\n",
      "3373/3373 [==============================] - 12s 3ms/step - loss: 0.5381 - acc: 0.7258 - val_loss: 0.5088 - val_acc: 0.7303\n",
      "Epoch 21/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5273 - acc: 0.7281\n",
      "Epoch 00021: val_loss did not improve from 0.50788\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.5273 - acc: 0.7281 - val_loss: 0.5144 - val_acc: 0.7342\n",
      "Epoch 22/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5222 - acc: 0.7286\n",
      "Epoch 00022: val_loss improved from 0.50788 to 0.50367, saving model to Step1_3_Epoch-22\n",
      "INFO:tensorflow:Assets written to: Step1_3_Epoch-22/assets\n",
      "3373/3373 [==============================] - 13s 4ms/step - loss: 0.5222 - acc: 0.7287 - val_loss: 0.5037 - val_acc: 0.7306\n",
      "Epoch 23/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5352 - acc: 0.7262\n",
      "Epoch 00023: val_loss did not improve from 0.50367\n",
      "3373/3373 [==============================] - 12s 3ms/step - loss: 0.5352 - acc: 0.7262 - val_loss: 0.5463 - val_acc: 0.7261\n",
      "Epoch 24/100\n",
      "3360/3373 [============================>.] - ETA: 0s - loss: 0.5269 - acc: 0.7291\n",
      "Epoch 00024: val_loss did not improve from 0.50367\n",
      "3373/3373 [==============================] - 12s 3ms/step - loss: 0.5268 - acc: 0.7292 - val_loss: 0.5268 - val_acc: 0.7299\n",
      "Epoch 25/100\n",
      "3359/3373 [============================>.] - ETA: 0s - loss: 0.5287 - acc: 0.7284\n",
      "Epoch 00025: val_loss did not improve from 0.50367\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.5286 - acc: 0.7284 - val_loss: 0.5106 - val_acc: 0.7189\n",
      "Epoch 26/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5193 - acc: 0.7295\n",
      "Epoch 00026: val_loss did not improve from 0.50367\n",
      "3373/3373 [==============================] - 12s 3ms/step - loss: 0.5193 - acc: 0.7295 - val_loss: 0.5065 - val_acc: 0.7294\n",
      "Epoch 27/100\n",
      "3371/3373 [============================>.] - ETA: 0s - loss: 0.5292 - acc: 0.7279\n",
      "Epoch 00027: val_loss did not improve from 0.50367\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.5292 - acc: 0.7279 - val_loss: 0.5094 - val_acc: 0.7278\n",
      "Epoch 28/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5209 - acc: 0.7306\n",
      "Epoch 00028: val_loss did not improve from 0.50367\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.5211 - acc: 0.7305 - val_loss: 0.5456 - val_acc: 0.7210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/100\n",
      "3360/3373 [============================>.] - ETA: 0s - loss: 0.5268 - acc: 0.7293\n",
      "Epoch 00029: val_loss improved from 0.50367 to 0.50062, saving model to Step1_3_Epoch-29\n",
      "INFO:tensorflow:Assets written to: Step1_3_Epoch-29/assets\n",
      "3373/3373 [==============================] - 13s 4ms/step - loss: 0.5267 - acc: 0.7293 - val_loss: 0.5006 - val_acc: 0.7336\n",
      "Epoch 30/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5203 - acc: 0.7307\n",
      "Epoch 00030: val_loss did not improve from 0.50062\n",
      "3373/3373 [==============================] - 12s 3ms/step - loss: 0.5203 - acc: 0.7307 - val_loss: 0.5190 - val_acc: 0.7285\n",
      "Epoch 31/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5174 - acc: 0.7307\n",
      "Epoch 00031: val_loss improved from 0.50062 to 0.49389, saving model to Step1_3_Epoch-31\n",
      "INFO:tensorflow:Assets written to: Step1_3_Epoch-31/assets\n",
      "3373/3373 [==============================] - 13s 4ms/step - loss: 0.5173 - acc: 0.7308 - val_loss: 0.4939 - val_acc: 0.7402\n",
      "Epoch 32/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5181 - acc: 0.7312\n",
      "Epoch 00032: val_loss did not improve from 0.49389\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.5181 - acc: 0.7312 - val_loss: 0.4977 - val_acc: 0.7403\n",
      "Epoch 33/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5205 - acc: 0.7309\n",
      "Epoch 00033: val_loss did not improve from 0.49389\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.5204 - acc: 0.7309 - val_loss: 0.4953 - val_acc: 0.7442\n",
      "Epoch 34/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5146 - acc: 0.7317\n",
      "Epoch 00034: val_loss did not improve from 0.49389\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.5146 - acc: 0.7318 - val_loss: 0.4947 - val_acc: 0.7383\n",
      "Epoch 35/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5148 - acc: 0.7319\n",
      "Epoch 00035: val_loss did not improve from 0.49389\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.5147 - acc: 0.7320 - val_loss: 0.5025 - val_acc: 0.7375\n",
      "Epoch 36/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5091 - acc: 0.7335\n",
      "Epoch 00036: val_loss did not improve from 0.49389\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.5091 - acc: 0.7335 - val_loss: 0.4978 - val_acc: 0.7342\n",
      "Epoch 37/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5251 - acc: 0.7307\n",
      "Epoch 00037: val_loss did not improve from 0.49389\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.5250 - acc: 0.7308 - val_loss: 0.5050 - val_acc: 0.7290\n",
      "Epoch 38/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5154 - acc: 0.7326\n",
      "Epoch 00038: val_loss did not improve from 0.49389\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.5153 - acc: 0.7326 - val_loss: 0.4977 - val_acc: 0.7362\n",
      "Epoch 39/100\n",
      "3360/3373 [============================>.] - ETA: 0s - loss: 0.5115 - acc: 0.7336\n",
      "Epoch 00039: val_loss did not improve from 0.49389\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.5114 - acc: 0.7336 - val_loss: 0.4988 - val_acc: 0.7299\n",
      "Epoch 40/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5126 - acc: 0.7332\n",
      "Epoch 00040: val_loss did not improve from 0.49389\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.5127 - acc: 0.7331 - val_loss: 0.5191 - val_acc: 0.7329\n",
      "Epoch 41/100\n",
      "3361/3373 [============================>.] - ETA: 0s - loss: 0.5105 - acc: 0.7343\n",
      "Epoch 00041: val_loss did not improve from 0.49389\n",
      "Restoring model weights from the end of the best epoch.\n",
      "3373/3373 [==============================] - 12s 4ms/step - loss: 0.5104 - acc: 0.7344 - val_loss: 0.5134 - val_acc: 0.7319\n",
      "Epoch 00041: early stopping\n",
      "Epoch 1/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 1.4610 - acc: 0.5000\n",
      "Epoch 00001: val_loss improved from inf to 10.73534, saving model to Step2_3_Epoch-1\n",
      "INFO:tensorflow:Assets written to: Step2_3_Epoch-1/assets\n",
      "3323/3323 [==============================] - 48s 14ms/step - loss: 1.4603 - acc: 0.5000 - val_loss: 10.7353 - val_acc: 0.5000\n",
      "Epoch 2/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 1.3064 - acc: 0.5004\n",
      "Epoch 00002: val_loss did not improve from 10.73534\n",
      "3323/3323 [==============================] - 12s 3ms/step - loss: 1.3061 - acc: 0.5004 - val_loss: 19.8235 - val_acc: 0.5000\n",
      "Epoch 3/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 1.2089 - acc: 0.5000\n",
      "Epoch 00003: val_loss did not improve from 10.73534\n",
      "3323/3323 [==============================] - 12s 3ms/step - loss: 1.2076 - acc: 0.5000 - val_loss: 17.1933 - val_acc: 0.5000\n",
      "Epoch 4/100\n",
      "3312/3323 [============================>.] - ETA: 0s - loss: 1.2318 - acc: 0.4996\n",
      "Epoch 00004: val_loss did not improve from 10.73534\n",
      "3323/3323 [==============================] - 12s 3ms/step - loss: 1.2303 - acc: 0.4996 - val_loss: 33.9336 - val_acc: 0.5000\n",
      "Epoch 5/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 1.1633 - acc: 0.5005\n",
      "Epoch 00005: val_loss did not improve from 10.73534\n",
      "3323/3323 [==============================] - 12s 3ms/step - loss: 1.1623 - acc: 0.5005 - val_loss: 15.5347 - val_acc: 0.5000\n",
      "Epoch 6/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 1.1404 - acc: 0.4993\n",
      "Epoch 00006: val_loss did not improve from 10.73534\n",
      "3323/3323 [==============================] - 12s 3ms/step - loss: 1.1396 - acc: 0.4993 - val_loss: 12.4408 - val_acc: 0.5000\n",
      "Epoch 7/100\n",
      "3317/3323 [============================>.] - ETA: 0s - loss: 1.0932 - acc: 0.4999\n",
      "Epoch 00007: val_loss did not improve from 10.73534\n",
      "3323/3323 [==============================] - 12s 4ms/step - loss: 1.0928 - acc: 0.4999 - val_loss: 18.3212 - val_acc: 0.5000\n",
      "Epoch 8/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 1.0939 - acc: 0.5006\n",
      "Epoch 00008: val_loss did not improve from 10.73534\n",
      "3323/3323 [==============================] - 12s 3ms/step - loss: 1.0929 - acc: 0.5005 - val_loss: 17.6584 - val_acc: 0.5000\n",
      "Epoch 9/100\n",
      "3312/3323 [============================>.] - ETA: 0s - loss: 1.0559 - acc: 0.4999\n",
      "Epoch 00009: val_loss did not improve from 10.73534\n",
      "3323/3323 [==============================] - 12s 3ms/step - loss: 1.0550 - acc: 0.4999 - val_loss: 11.2035 - val_acc: 0.5000\n",
      "Epoch 10/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 1.0873 - acc: 0.4995\n",
      "Epoch 00010: val_loss improved from 10.73534 to 8.57717, saving model to Step2_3_Epoch-10\n",
      "INFO:tensorflow:Assets written to: Step2_3_Epoch-10/assets\n",
      "3323/3323 [==============================] - 12s 4ms/step - loss: 1.0868 - acc: 0.4995 - val_loss: 8.5772 - val_acc: 0.5000\n",
      "Epoch 11/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 1.1391 - acc: 0.5004\n",
      "Epoch 00011: val_loss improved from 8.57717 to 7.38232, saving model to Step2_3_Epoch-11\n",
      "INFO:tensorflow:Assets written to: Step2_3_Epoch-11/assets\n",
      "3323/3323 [==============================] - 12s 4ms/step - loss: 1.1383 - acc: 0.5004 - val_loss: 7.3823 - val_acc: 0.5000\n",
      "Epoch 12/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 1.0652 - acc: 0.5000\n",
      "Epoch 00012: val_loss did not improve from 7.38232\n",
      "3323/3323 [==============================] - 12s 3ms/step - loss: 1.0645 - acc: 0.4999 - val_loss: 50.1350 - val_acc: 0.5000\n",
      "Epoch 13/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 1.4014 - acc: 0.4999\n",
      "Epoch 00013: val_loss did not improve from 7.38232\n",
      "3323/3323 [==============================] - 12s 3ms/step - loss: 1.3996 - acc: 0.4999 - val_loss: 39.9928 - val_acc: 0.5000\n",
      "Epoch 14/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 1.0810 - acc: 0.5000\n",
      "Epoch 00014: val_loss improved from 7.38232 to 6.21912, saving model to Step2_3_Epoch-14\n",
      "INFO:tensorflow:Assets written to: Step2_3_Epoch-14/assets\n",
      "3323/3323 [==============================] - 12s 4ms/step - loss: 1.0803 - acc: 0.5000 - val_loss: 6.2191 - val_acc: 0.5000\n",
      "Epoch 15/100\n",
      "3311/3323 [============================>.] - ETA: 0s - loss: 1.0586 - acc: 0.5001\n",
      "Epoch 00015: val_loss did not improve from 6.21912\n",
      "3323/3323 [==============================] - 12s 3ms/step - loss: 1.0574 - acc: 0.5001 - val_loss: 33.4457 - val_acc: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 1.1681 - acc: 0.5001\n",
      "Epoch 00016: val_loss did not improve from 6.21912\n",
      "3323/3323 [==============================] - 12s 3ms/step - loss: 1.1671 - acc: 0.5001 - val_loss: 13.7453 - val_acc: 0.5000\n",
      "Epoch 17/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 1.2153 - acc: 0.5001\n",
      "Epoch 00017: val_loss did not improve from 6.21912\n",
      "3323/3323 [==============================] - 12s 4ms/step - loss: 1.2143 - acc: 0.5001 - val_loss: 16.8433 - val_acc: 0.5000\n",
      "Epoch 18/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 1.0321 - acc: 0.5004\n",
      "Epoch 00018: val_loss did not improve from 6.21912\n",
      "3323/3323 [==============================] - 12s 4ms/step - loss: 1.0316 - acc: 0.5004 - val_loss: 6.7806 - val_acc: 0.5000\n",
      "Epoch 19/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 1.0125 - acc: 0.5001\n",
      "Epoch 00019: val_loss did not improve from 6.21912\n",
      "3323/3323 [==============================] - 12s 4ms/step - loss: 1.0117 - acc: 0.5001 - val_loss: 24.3823 - val_acc: 0.5000\n",
      "Epoch 20/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 1.3999 - acc: 0.4993\n",
      "Epoch 00020: val_loss did not improve from 6.21912\n",
      "3323/3323 [==============================] - 12s 4ms/step - loss: 1.4000 - acc: 0.4993 - val_loss: 9.2302 - val_acc: 0.5000\n",
      "Epoch 21/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 1.0781 - acc: 0.5002\n",
      "Epoch 00021: val_loss improved from 6.21912 to 3.56063, saving model to Step2_3_Epoch-21\n",
      "INFO:tensorflow:Assets written to: Step2_3_Epoch-21/assets\n",
      "3323/3323 [==============================] - 12s 4ms/step - loss: 1.0800 - acc: 0.5002 - val_loss: 3.5606 - val_acc: 0.5000\n",
      "Epoch 22/100\n",
      "3315/3323 [============================>.] - ETA: 0s - loss: 1.3172 - acc: 0.4996\n",
      "Epoch 00022: val_loss did not improve from 3.56063\n",
      "3323/3323 [==============================] - 12s 4ms/step - loss: 1.3175 - acc: 0.4996 - val_loss: 12.6237 - val_acc: 0.5000\n",
      "Epoch 23/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 1.2529 - acc: 0.4997\n",
      "Epoch 00023: val_loss did not improve from 3.56063\n",
      "3323/3323 [==============================] - 12s 4ms/step - loss: 1.2518 - acc: 0.4997 - val_loss: 5.0419 - val_acc: 0.5000\n",
      "Epoch 24/100\n",
      "3312/3323 [============================>.] - ETA: 0s - loss: 1.0807 - acc: 0.4995\n",
      "Epoch 00024: val_loss did not improve from 3.56063\n",
      "3323/3323 [==============================] - 12s 4ms/step - loss: 1.0796 - acc: 0.4995 - val_loss: 29.1030 - val_acc: 0.5000\n",
      "Epoch 25/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 1.0976 - acc: 0.5002\n",
      "Epoch 00025: val_loss did not improve from 3.56063\n",
      "3323/3323 [==============================] - 12s 4ms/step - loss: 1.0967 - acc: 0.5001 - val_loss: 14.3736 - val_acc: 0.5000\n",
      "Epoch 26/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 1.1691 - acc: 0.4994\n",
      "Epoch 00026: val_loss did not improve from 3.56063\n",
      "3323/3323 [==============================] - 12s 3ms/step - loss: 1.1682 - acc: 0.4994 - val_loss: 7.2442 - val_acc: 0.5000\n",
      "Epoch 27/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 1.1339 - acc: 0.4996\n",
      "Epoch 00027: val_loss did not improve from 3.56063\n",
      "3323/3323 [==============================] - 12s 3ms/step - loss: 1.1331 - acc: 0.4996 - val_loss: 6.6235 - val_acc: 0.5000\n",
      "Epoch 28/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 1.0094 - acc: 0.4995\n",
      "Epoch 00028: val_loss did not improve from 3.56063\n",
      "3323/3323 [==============================] - 12s 3ms/step - loss: 1.0088 - acc: 0.4996 - val_loss: 7.4181 - val_acc: 0.5000\n",
      "Epoch 29/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 1.2893 - acc: 0.4996\n",
      "Epoch 00029: val_loss did not improve from 3.56063\n",
      "3323/3323 [==============================] - 12s 4ms/step - loss: 1.2881 - acc: 0.4996 - val_loss: 4.6058 - val_acc: 0.5000\n",
      "Epoch 30/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 0.9721 - acc: 0.4998\n",
      "Epoch 00030: val_loss did not improve from 3.56063\n",
      "3323/3323 [==============================] - 12s 3ms/step - loss: 0.9718 - acc: 0.4998 - val_loss: 9.0020 - val_acc: 0.5000\n",
      "Epoch 31/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 1.0167 - acc: 0.4999\n",
      "Epoch 00031: val_loss improved from 3.56063 to 2.42487, saving model to Step2_3_Epoch-31\n",
      "INFO:tensorflow:Assets written to: Step2_3_Epoch-31/assets\n",
      "3323/3323 [==============================] - 12s 4ms/step - loss: 1.0162 - acc: 0.4999 - val_loss: 2.4249 - val_acc: 0.5000\n",
      "Epoch 32/100\n",
      "3317/3323 [============================>.] - ETA: 0s - loss: 1.3019 - acc: 0.5003\n",
      "Epoch 00032: val_loss improved from 2.42487 to 2.17002, saving model to Step2_3_Epoch-32\n",
      "INFO:tensorflow:Assets written to: Step2_3_Epoch-32/assets\n",
      "3323/3323 [==============================] - 12s 4ms/step - loss: 1.3012 - acc: 0.5003 - val_loss: 2.1700 - val_acc: 0.5000\n",
      "Epoch 33/100\n",
      "3312/3323 [============================>.] - ETA: 0s - loss: 1.0704 - acc: 0.4997\n",
      "Epoch 00033: val_loss improved from 2.17002 to 2.13160, saving model to Step2_3_Epoch-33\n",
      "INFO:tensorflow:Assets written to: Step2_3_Epoch-33/assets\n",
      "3323/3323 [==============================] - 12s 4ms/step - loss: 1.0698 - acc: 0.4997 - val_loss: 2.1316 - val_acc: 0.5000\n",
      "Epoch 34/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 1.0082 - acc: 0.4996\n",
      "Epoch 00034: val_loss did not improve from 2.13160\n",
      "3323/3323 [==============================] - 12s 3ms/step - loss: 1.0076 - acc: 0.4996 - val_loss: 4.1361 - val_acc: 0.5000\n",
      "Epoch 35/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 0.9704 - acc: 0.4998\n",
      "Epoch 00035: val_loss did not improve from 2.13160\n",
      "3323/3323 [==============================] - 12s 3ms/step - loss: 0.9698 - acc: 0.4998 - val_loss: 20.9097 - val_acc: 0.5000\n",
      "Epoch 36/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 1.0509 - acc: 0.4999\n",
      "Epoch 00036: val_loss did not improve from 2.13160\n",
      "3323/3323 [==============================] - 12s 3ms/step - loss: 1.0508 - acc: 0.4999 - val_loss: 4.2607 - val_acc: 0.5000\n",
      "Epoch 37/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 1.0744 - acc: 0.4998\n",
      "Epoch 00037: val_loss did not improve from 2.13160\n",
      "3323/3323 [==============================] - 12s 3ms/step - loss: 1.0735 - acc: 0.4998 - val_loss: 6.3723 - val_acc: 0.5000\n",
      "Epoch 38/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 1.0773 - acc: 0.4996\n",
      "Epoch 00038: val_loss did not improve from 2.13160\n",
      "3323/3323 [==============================] - 12s 3ms/step - loss: 1.0773 - acc: 0.4996 - val_loss: 2.1400 - val_acc: 0.5000\n",
      "Epoch 39/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 1.0927 - acc: 0.5000\n",
      "Epoch 00039: val_loss did not improve from 2.13160\n",
      "3323/3323 [==============================] - 12s 3ms/step - loss: 1.0924 - acc: 0.5001 - val_loss: 3.2255 - val_acc: 0.5000\n",
      "Epoch 40/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 0.9392 - acc: 0.5001\n",
      "Epoch 00040: val_loss did not improve from 2.13160\n",
      "3323/3323 [==============================] - 12s 4ms/step - loss: 0.9385 - acc: 0.5001 - val_loss: 4.3549 - val_acc: 0.5000\n",
      "Epoch 41/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 1.2244 - acc: 0.4996\n",
      "Epoch 00041: val_loss did not improve from 2.13160\n",
      "3323/3323 [==============================] - 12s 3ms/step - loss: 1.2235 - acc: 0.4996 - val_loss: 4.9004 - val_acc: 0.5000\n",
      "Epoch 42/100\n",
      "3316/3323 [============================>.] - ETA: 0s - loss: 1.0176 - acc: 0.5002\n",
      "Epoch 00042: val_loss improved from 2.13160 to 1.38874, saving model to Step2_3_Epoch-42\n",
      "INFO:tensorflow:Assets written to: Step2_3_Epoch-42/assets\n",
      "3323/3323 [==============================] - 12s 4ms/step - loss: 1.0175 - acc: 0.5002 - val_loss: 1.3887 - val_acc: 0.5000\n",
      "Epoch 43/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 0.9354 - acc: 0.4999\n",
      "Epoch 00043: val_loss did not improve from 1.38874\n",
      "3323/3323 [==============================] - 12s 3ms/step - loss: 0.9348 - acc: 0.4999 - val_loss: 2.2582 - val_acc: 0.5000\n",
      "Epoch 44/100\n",
      "3312/3323 [============================>.] - ETA: 0s - loss: 1.0796 - acc: 0.4999\n",
      "Epoch 00044: val_loss did not improve from 1.38874\n",
      "3323/3323 [==============================] - 12s 3ms/step - loss: 1.0787 - acc: 0.5000 - val_loss: 1.7839 - val_acc: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 1.0043 - acc: 0.5002\n",
      "Epoch 00045: val_loss did not improve from 1.38874\n",
      "3323/3323 [==============================] - 12s 3ms/step - loss: 1.0034 - acc: 0.5002 - val_loss: 10.6700 - val_acc: 0.5000\n",
      "Epoch 46/100\n",
      "3311/3323 [============================>.] - ETA: 0s - loss: 1.0314 - acc: 0.4999\n",
      "Epoch 00046: val_loss did not improve from 1.38874\n",
      "3323/3323 [==============================] - 12s 3ms/step - loss: 1.0323 - acc: 0.4998 - val_loss: 2.2265 - val_acc: 0.5000\n",
      "Epoch 47/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 0.9696 - acc: 0.5001\n",
      "Epoch 00047: val_loss did not improve from 1.38874\n",
      "3323/3323 [==============================] - 12s 3ms/step - loss: 0.9690 - acc: 0.5001 - val_loss: 9.5692 - val_acc: 0.5000\n",
      "Epoch 48/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 1.0009 - acc: 0.4996\n",
      "Epoch 00048: val_loss did not improve from 1.38874\n",
      "3323/3323 [==============================] - 12s 4ms/step - loss: 1.0001 - acc: 0.4996 - val_loss: 5.8649 - val_acc: 0.5000\n",
      "Epoch 49/100\n",
      "3312/3323 [============================>.] - ETA: 0s - loss: 1.2250 - acc: 0.4995\n",
      "Epoch 00049: val_loss did not improve from 1.38874\n",
      "3323/3323 [==============================] - 12s 3ms/step - loss: 1.2240 - acc: 0.4995 - val_loss: 3.8013 - val_acc: 0.5000\n",
      "Epoch 50/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 0.9291 - acc: 0.4996\n",
      "Epoch 00050: val_loss did not improve from 1.38874\n",
      "3323/3323 [==============================] - 12s 3ms/step - loss: 0.9285 - acc: 0.4996 - val_loss: 13.2220 - val_acc: 0.5000\n",
      "Epoch 51/100\n",
      "3313/3323 [============================>.] - ETA: 0s - loss: 0.9352 - acc: 0.4999\n",
      "Epoch 00051: val_loss did not improve from 1.38874\n",
      "3323/3323 [==============================] - 12s 3ms/step - loss: 0.9348 - acc: 0.4999 - val_loss: 12.4624 - val_acc: 0.5000\n",
      "Epoch 52/100\n",
      "3311/3323 [============================>.] - ETA: 0s - loss: 1.0040 - acc: 0.5004\n",
      "Epoch 00052: val_loss did not improve from 1.38874\n",
      "Restoring model weights from the end of the best epoch.\n",
      "3323/3323 [==============================] - 12s 4ms/step - loss: 1.0030 - acc: 0.5004 - val_loss: 67.3319 - val_acc: 0.5000\n",
      "Epoch 00052: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-31 16:18:55.910816: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-31 16:18:55.916297: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2599945000 Hz\n",
      "2022-05-31 16:18:55.916415: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55a9a93b0e60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-05-31 16:18:55.916432: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2022-05-31 16:22:11.898561: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    }
   ],
   "source": [
    "# apply the OmniFold procedure to get weights for the generation\n",
    "multifold_ws = omnifold.omnifold(X_gen, Y_gen, X_det, Y_det, wdata, winit,\n",
    "                                (ef.archs.DNN, det_args), (ef.archs.DNN, mc_args),\n",
    "                                fitargs, val=0.2, it=itnum, trw_ind=-2, weights_filename='5311618')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def axes(ratio_plot=True, figsize=(4,4), gridspec_update=None,\n",
    "         xlabel='', ylabel=r'Probability Density', ylabel_ratio='Ratio to\\nTruth', units='', \n",
    "         xlim=(0,1), ylim=(0,1), ylim_ratio=(0.5,1.5),\n",
    "         xticks=None, yticks=None, xtick_step=None, ytick_step=None, ytick_ratio_step=0.25,\n",
    "         **kwargs):\n",
    "    \n",
    "    # gridspec options\n",
    "    gridspec_kw = {'height_ratios': (3.5, 1) if ratio_plot else (1,), 'hspace': 0.0}\n",
    "    if isinstance(gridspec_update, dict):\n",
    "        gridspec_kw.update(gridspec_update)\n",
    "\n",
    "    # get subplots\n",
    "    nsubplots = 2 if ratio_plot else 1\n",
    "    fig, axes = plt.subplots(nsubplots,  gridspec_kw=gridspec_kw, figsize=figsize)\n",
    "    if nsubplots == 1:\n",
    "        axes = [axes]\n",
    "        \n",
    "    # axes limits\n",
    "    for ax in axes:\n",
    "        ax.set_xlim(*xlim)\n",
    "    axes[0].set_ylim(*ylim)\n",
    "    #axes[0].set_yscale('log')\n",
    "    #axes[0].set_ylim(bottom=0.00001, top=2)\n",
    "    if ratio_plot:\n",
    "        axes[1].set_ylim(*ylim_ratio)\n",
    "        \n",
    "    # axes labels\n",
    "    if units:\n",
    "        xlabel = r'{} [{}]'.format(xlabel, units)\n",
    "        ylabel = r'{} [{}{}]'.format(ylabel, units, r'$^{-1}$')\n",
    "    axes[-1].set_xlabel(xlabel)\n",
    "    axes[0].set_ylabel(ylabel)\n",
    "    if ratio_plot:\n",
    "        axes[1].set_ylabel(ylabel_ratio, fontsize=8)\n",
    "        \n",
    "    # tick settings\n",
    "    for ax in axes:\n",
    "        ax.minorticks_on()\n",
    "        ax.tick_params(top=True, right=True, bottom=True, left=True, direction='in', which='both')\n",
    "    if ratio_plot:\n",
    "        axes[0].tick_params(labelbottom=False)\n",
    "        axes[1].tick_params(axis='y', labelsize=8)\n",
    "    \n",
    "    # tick locations and labels\n",
    "    if xtick_step is not None:\n",
    "        xticks_locs = [round(xlim[0] + i*xtick_step, 4) for i in range(1+math.floor((xlim[1]-xlim[0])/xtick_step))]\n",
    "        axes[-1].set_xticks(xticks_locs)\n",
    "        if xticks is None:\n",
    "            axes[-1].set_xticklabels(list(map(str, xticks_locs)))\n",
    "        else:\n",
    "            axes[-1].set_xticklabels(xticks)\n",
    "    if ytick_step is not None:\n",
    "        yticks_locs = [round(ylim[0] + i*ytick_step, 4) for i in range(1+math.floor((ylim[1]-ylim[0])/ytick_step))]\n",
    "        axes[0].set_yticks(yticks_locs)\n",
    "        if yticks is None:\n",
    "            axes[0].set_yticklabels(list(map(str, yticks_locs)))\n",
    "        else:\n",
    "            axes[0].set_yticklabels(yticks)\n",
    "    if ytick_ratio_step is not None and ratio_plot:\n",
    "        yticks = [round(ylim_ratio[0] + i*ytick_ratio_step, 4)\n",
    "                  for i in range(1+round((ylim_ratio[1]-ylim_ratio[0])/ytick_ratio_step))][1:-1]\n",
    "        axes[1].set_yticks(yticks)\n",
    "        axes[1].set_yticklabels(list(map(str, yticks)))\n",
    "    \n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0375 0.1125 0.1875 0.2625 0.3375 0.4125 0.4875 0.5625 0.6375 0.7125\n",
      " 0.7875 0.8625 0.9375 1.0125 1.0875 1.1625 1.2375 1.3125 1.3875 1.4625]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 480x480 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.525 0.575 0.625 0.675 0.725 0.775 0.825 0.875 0.925 0.975 1.025 1.075\n",
      " 1.125 1.175 1.225 1.275 1.325 1.375 1.425 1.475]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 480x480 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i,(obkey,ob) in enumerate(obs.items()):\n",
    "    \n",
    "    # get the styled axes on which to plot\n",
    "    fig, [ax0, ax1] = axes(**ob)\n",
    "    if ob.get('yscale') is not None:\n",
    "        ax0.set_yscale(ob['yscale'])\n",
    "    #ax0.set_yscale('log')\n",
    "\n",
    "        \n",
    "    # Plot the Different Distributions of the Observable\n",
    "    # plot the \"data\" histogram of the observable\n",
    "    ax0.hist(ob['dataobs'], bins=ob['bins_det'], color='black', label='HERWIG det', **hist_style)\n",
    "\n",
    "    # plot the \"sim\" histogram of the observable\n",
    "    ax0.hist(ob['simobs'], bins=ob['bins_det'], color='orange', label='PYTHIA det', **hist_style)\n",
    "\n",
    "    # plot the \"gen\" histogram of the observable\n",
    "    ax0.plot(ob['midbins_mc'], ob['genobs_hist'], **gen_style)\n",
    "\n",
    "    # plot the \"truth\" histogram of the observable\n",
    "    ax0.fill_between(ob['midbins_mc'], ob['truth_hist'], **truth_style)\n",
    "\n",
    "    \n",
    "    # Plot the Unfolded Distributions of the Observable\n",
    "    # plot the OmniFold distribution\n",
    "    of_histgen, of_histgen_unc = modplot.calc_hist(ob['genobs'], weights=multifold_ws[2*itnum], \n",
    "                                                   bins=ob['bins_mc'], density=True)[:2]\n",
    "    ax0.plot(ob['midbins_mc'], of_histgen, **omnifold_style, label='MultiFold')\n",
    "           \n",
    "    # plot the IBU distribution\n",
    "    ax0.plot(ob['midbins_mc'], ob['ibu_phis'][itnum], **ibu_style, label='IBU ' + ob['symbol'])\n",
    "\n",
    "    # Plot the Ratios of the OmniFold and IBU distributions to truth (with statistical uncertainties)\n",
    "    ibu_ratio = ob['ibu_phis'][itnum]/(ob['truth_hist'] + 10**-50)\n",
    "    of_ratio = of_histgen/(ob['truth_hist'] + 10**-50)\n",
    "    ax1.plot([np.min(ob['midbins_mc']), np.max(ob['midbins_mc'])], [1, 1], '-', color='green', lw=0.75)\n",
    "    \n",
    "    # ratio uncertainties\n",
    "    truth_unc_ratio = ob['truth_hist_unc']/(ob['truth_hist'] + 10**-50)\n",
    "    ibu_unc_ratio = ob['ibu_phi_unc']/(ob['truth_hist'] + 10**-50)\n",
    "    of_unc_ratio = of_histgen_unc/(ob['truth_hist'] + 10**-50)\n",
    "    \n",
    "    ax1.fill_between(ob['midbins_mc'], 1 - truth_unc_ratio, 1 + truth_unc_ratio, \n",
    "                     facecolor=truth_style['facecolor'], zorder=-2)\n",
    "    ax1.errorbar(ob['midbins_mc'], ibu_ratio, xerr=ob['binwidth_mc']/2, yerr=ibu_unc_ratio, \n",
    "                                              color=ibu_style['color'], **modplot.style('errorbar'))\n",
    "    ax1.errorbar(ob['midbins_mc'], of_ratio, xerr=ob['binwidth_mc']/2, yerr=of_unc_ratio, \n",
    "                                              color=omnifold_style['color'], **modplot.style('errorbar'))\n",
    "    \n",
    "    print(ob['midbins_mc'])\n",
    "    # legend style and ordering\n",
    "    loc, ncol = ob.get('legend_loc', 'upper right'), ob.get('legend_ncol', 2)\n",
    "    order = [3, 4, 2, 5, 0, 1] if ncol==2 else [3, 5, 4, 0, 2, 1]\n",
    "    modplot.legend(ax=ax0, frameon=False, order=order, loc=loc, ncol=ncol)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
